# LLM-Aware Gateway æŠ€æœ¯æ–¹æ¡ˆ

> è¯­ä¹‰æ„ŸçŸ¥ç†”æ–­/é™æµç½‘å…³çš„è¯¦ç»†æŠ€æœ¯å®ç°æ–¹æ¡ˆ

## ğŸ“‹ æŠ€æœ¯æ–¹æ¡ˆæ¦‚è§ˆ

### æ–¹æ¡ˆç›®æ ‡
æ„å»ºä¸€ä¸ªåŸºäºè¯­ä¹‰åˆ†æçš„æ™ºèƒ½ç½‘å…³ç³»ç»Ÿï¼Œé€šè¿‡å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯è¯†åˆ«åŒç±»é”™è¯¯å¹¶å®ç°ç²¾ç¡®ç†”æ–­é™æµï¼Œç›¸æ¯”ä¼ ç»ŸåŸºäºQPS/é”™è¯¯ç‡çš„ç²—ç²’åº¦ç­–ç•¥ï¼Œæ˜¾è‘—é™ä½è¯¯æ€ç‡å¹¶æå‡ç³»ç»Ÿç¨³å®šæ€§ã€‚

### æŠ€æœ¯é€‰å‹

| ç»„ä»¶ç±»åˆ« | æŠ€æœ¯é€‰å‹ | è¯´æ˜ |
|---------|----------|------|
| ç¼–ç¨‹è¯­è¨€ | Go 1.21+ | é«˜æ€§èƒ½ã€å¹¶å‘å‹å¥½ |
| ç½‘å…³æ¡†æ¶ | Gin + è‡ªç ”ä¸­é—´ä»¶ | è½»é‡çº§ã€å¯æ‰©å±• |
| å‘é‡æ•°æ®åº“ | FAISS + pgvector | FAISSå†…å­˜è®¡ç®—ï¼ŒpgvectoræŒä¹…åŒ– |
| æ¶ˆæ¯é˜Ÿåˆ— | Kafka | å¼‚æ­¥æ•°æ®ä¼ è¾“ |
| é…ç½®ä¸­å¿ƒ | ETCD | ç­–ç•¥é…ç½®çƒ­æ›´æ–° |
| ç›‘æ§ç³»ç»Ÿ | Prometheus + Grafana | æŒ‡æ ‡ç›‘æ§ |
| é“¾è·¯è¿½è¸ª | OpenTelemetry | åˆ†å¸ƒå¼è¿½è¸ª |
| åµŒå…¥æ¨¡å‹ | BGE-small-zh | è½»é‡ä¸­æ–‡å‘é‡åŒ–æ¨¡å‹ |

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å®¢æˆ·ç«¯å±‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   è´Ÿè½½å‡è¡¡å±‚                                  â”‚
â”‚              Nginx/Envoy/CloudFlare                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®é¢ (Gateway)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   è·¯ç”±æ¨¡å—   â”‚  â”‚   é‰´æƒæ¨¡å—   â”‚  â”‚    é™æµ/ç†”æ–­æ¨¡å—     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  é”™è¯¯é‡‡æ ·å™¨  â”‚  â”‚ å‘é‡åŒ–Agent â”‚  â”‚     æŒ‡æ ‡é‡‡é›†å™¨       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ (Kafkaæ¶ˆæ¯é˜Ÿåˆ—)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ§åˆ¶é¢ (Control Plane)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  åµŒå…¥æœåŠ¡    â”‚  â”‚  å‘é‡æ•°æ®åº“  â”‚  â”‚     èšç±»å¼•æ“         â”‚  â”‚
â”‚  â”‚ BGE-small   â”‚  â”‚FAISS+pgvec â”‚  â”‚   DBSCAN/HNSW       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ç­–ç•¥å¼•æ“    â”‚  â”‚  é…ç½®ä¸­å¿ƒ    â”‚  â”‚     ç›‘æ§ä¸­å¿ƒ         â”‚  â”‚
â”‚  â”‚ è§„åˆ™ç”Ÿæˆ     â”‚  â”‚   ETCD      â”‚  â”‚  Prometheus         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ•°æ®æµè½¬æ¶æ„

```
è¯·æ±‚æµç¨‹ï¼š
Client â†’ Gateway â†’ è·¯ç”±/é‰´æƒ â†’ é™æµæ£€æŸ¥ â†’ ç†”æ–­æ£€æŸ¥ â†’ Downstream
              â†“
         é”™è¯¯é‡‡æ · â†’ Kafka â†’ æ§åˆ¶é¢å¤„ç† â†’ ç­–ç•¥ç”Ÿæˆ â†’ ETCD â†’ Gatewayæ›´æ–°

å‘é‡åŒ–æµç¨‹ï¼š
é”™è¯¯æ–‡æœ¬ â†’ æ–‡æœ¬é¢„å¤„ç† â†’ åµŒå…¥æ¨¡å‹ â†’ å‘é‡ â†’ FAISSç´¢å¼• â†’ èšç±»åˆ†æ
```

### ç³»ç»Ÿäº¤äº’è®¾è®¡

#### æœåŠ¡é—´é€šä¿¡æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant Gateway as ç½‘å…³(æ•°æ®é¢)
    participant Kafka as æ¶ˆæ¯é˜Ÿåˆ—
    participant Control as æ§åˆ¶é¢
    participant ETCD as é…ç½®ä¸­å¿ƒ
    participant Vector as å‘é‡æ•°æ®åº“
    participant Downstream as ä¸‹æ¸¸æœåŠ¡

    Client->>Gateway: HTTPè¯·æ±‚
    Gateway->>Gateway: é‰´æƒ/è·¯ç”±æ£€æŸ¥
    Gateway->>Gateway: é™æµæ£€æŸ¥
    Gateway->>Gateway: ç†”æ–­æ£€æŸ¥

    alt è¯·æ±‚æ­£å¸¸
        Gateway->>Downstream: è½¬å‘è¯·æ±‚
        Downstream-->>Gateway: å“åº”
        Gateway-->>Client: è¿”å›å“åº”
    else è¯·æ±‚å¼‚å¸¸
        Gateway->>Gateway: é”™è¯¯é‡‡æ ·
        Gateway->>Kafka: å¼‚æ­¥å‘é€é”™è¯¯äº‹ä»¶
        Gateway-->>Client: è¿”å›é”™è¯¯å“åº”

        Kafka->>Control: æ¶ˆè´¹é”™è¯¯äº‹ä»¶
        Control->>Control: æ–‡æœ¬å‘é‡åŒ–
        Control->>Vector: å­˜å‚¨å‘é‡
        Control->>Control: èšç±»åˆ†æ
        Control->>Control: ç­–ç•¥è¯„ä¼°
        Control->>ETCD: ä¸‹å‘ç­–ç•¥é…ç½®
        ETCD->>Gateway: é…ç½®æ›´æ–°é€šçŸ¥
        Gateway->>Gateway: æ›´æ–°é™æµ/ç†”æ–­ç­–ç•¥
    end
```

#### ç»„ä»¶ä¾èµ–å…³ç³»å›¾

```mermaid
graph TB
    subgraph "æ•°æ®é¢ (Data Plane)"
        GW[ç½‘å…³æœåŠ¡]
        RL[é™æµå™¨]
        CB[ç†”æ–­å™¨]
        ES[é”™è¯¯é‡‡æ ·å™¨]
        VA[å‘é‡Agent]
        CW[é…ç½®ç›‘å¬å™¨]
    end

    subgraph "æ§åˆ¶é¢ (Control Plane)"
        EM[åµŒå…¥æœåŠ¡]
        CE[èšç±»å¼•æ“]
        PE[ç­–ç•¥å¼•æ“]
        MC[æŒ‡æ ‡æ”¶é›†å™¨]
    end

    subgraph "å­˜å‚¨å±‚"
        VDB[(å‘é‡æ•°æ®åº“)]
        ETCD[(é…ç½®ä¸­å¿ƒ)]
        KAFKA[(æ¶ˆæ¯é˜Ÿåˆ—)]
        PG[(PostgreSQL)]
        REDIS[(Redisç¼“å­˜)]
    end

    subgraph "ç›‘æ§å±‚"
        PROM[Prometheus]
        GRAF[Grafana]
        ALERT[AlertManager]
    end

    GW --> RL
    GW --> CB
    GW --> ES
    GW --> VA
    GW --> CW

    ES --> KAFKA
    KAFKA --> CE
    CE --> EM
    CE --> VDB
    CE --> PE
    PE --> ETCD
    ETCD --> CW

    VDB --> PG
    VDB --> REDIS
    EM --> REDIS

    GW --> PROM
    CE --> PROM
    PE --> PROM
    PROM --> GRAF
    PROM --> ALERT
```

---

## ğŸ“ è¯¦ç»†è®¾è®¡

### ç±»å›¾è®¾è®¡

#### 1. ç½‘å…³æ ¸å¿ƒç±»å›¾

```mermaid
classDiagram
    class Gateway {
        -router: gin.Engine
        -rateLimiter: RateLimiter
        -circuitBreaker: CircuitBreaker
        -errorSampler: ErrorSampler
        -vectorAgent: VectorAgent
        -configWatcher: ConfigWatcher
        +setupMiddleware(): void
        +start(): error
        +stop(): error
    }

    class RateLimiter {
        -clusters: map[string]*ClusterLimiter
        -mutex: sync.RWMutex
        +Allow(ctx: gin.Context): bool
        +updatePolicy(clusterID: string, policy: Policy): void
    }

    class ClusterLimiter {
        -clusterID: string
        -tokenBucket: TokenBucket
        -severity: float64
        -baseRate: float64
        -currentRate: float64
        +allow(): bool
        +updateSeverity(severity: float64): void
    }

    class TokenBucket {
        -capacity: int64
        -tokens: int64
        -refillRate: float64
        -lastRefill: time.Time
        +allow(): bool
        +setRate(rate: float64): void
    }

    class CircuitBreaker {
        -clusters: map[string]*ClusterBreaker
        -mutex: sync.RWMutex
        +Allow(ctx: gin.Context, clusterID: string): bool
        +RecordSuccess(clusterID: string): void
        +RecordFailure(clusterID: string): void
    }

    class ClusterBreaker {
        -clusterID: string
        -state: BreakerState
        -failureCount: int64
        -successCount: int64
        -lastFailTime: time.Time
        -nextRetry: time.Time
        -config: BreakerConfig
        +allow(): bool
        +recordResult(success: bool): void
    }

    class ErrorSampler {
        -samplingRate: float64
        -kafkaProducer: kafka.Producer
        -desensitizer: Desensitizer
        +SampleError(ctx: gin.Context, err: error): void
        +sendToKafka(event: ErrorEvent): error
    }

    class VectorAgent {
        -embeddingService: EmbeddingService
        -cache: lru.Cache
        +IdentifyCluster(errorSignature: string): string
        +generateVector(text: string): []float32
    }

    Gateway --> RateLimiter
    Gateway --> CircuitBreaker
    Gateway --> ErrorSampler
    Gateway --> VectorAgent
    RateLimiter --> ClusterLimiter
    ClusterLimiter --> TokenBucket
    CircuitBreaker --> ClusterBreaker
```

#### 2. æ§åˆ¶é¢æ ¸å¿ƒç±»å›¾

```mermaid
classDiagram
    class ControlPlane {
        -embeddingService: EmbeddingService
        -clusteringEngine: ClusteringEngine
        -policyEngine: PolicyEngine
        -vectorDB: VectorDB
        -configStore: ConfigStore
        +start(): error
        +processErrorEvent(event: ErrorEvent): error
    }

    class EmbeddingService {
        -model: BGEModel
        -cache: lru.Cache
        -batchSize: int
        +EmbedText(text: string): []float32
        +EmbedBatch(texts: []string): [][]float32
        +preprocessText(text: string): string
    }

    class BGEModel {
        -modelPath: string
        -session: onnxruntime.Session
        +Encode(text: string): []float32
        +EncodeBatch(texts: []string): [][]float32
    }

    class ClusteringEngine {
        -vectorDB: VectorDB
        -clusterIndex: faiss.Index
        -clusters: map[string]*Cluster
        -similarityThreshold: float64
        -mutex: sync.RWMutex
        +ProcessErrorEvent(event: ErrorEvent): error
        +findMostSimilarCluster(vector: []float32): (string, float64)
        +createNewCluster(event: ErrorEvent, vector: []float32): string
        +ReCluster(): void
    }

    class Cluster {
        -id: string
        -centroid: []float32
        -members: []string
        -errorCount: int64
        -createTime: time.Time
        -updateTime: time.Time
        -severity: float64
        +addMember(memberID: string): void
        +updateCentroid(vectors: [][]float32): void
        +calculateSeverity(): float64
    }

    class PolicyEngine {
        -clusters: map[string]*Cluster
        -policies: map[string]*Policy
        -configStore: ConfigStore
        -metrics: MetricsCollector
        +EvaluatePolicies(): void
        +generatePolicy(cluster: Cluster): Policy
        +applyPolicy(policy: Policy): void
        +shouldTriggerPolicy(errorRate, growthRate: float64): bool
    }

    class Policy {
        -clusterID: string
        -policyType: PolicyType
        -severity: float64
        -rateLimit: RateLimitPolicy
        -circuitBreak: CircuitBreakPolicy
        -createTime: time.Time
        -expireTime: time.Time
    }

    class VectorDB {
        -faissIndex: faiss.Index
        -pgConn: sql.DB
        -cache: lru.Cache
        +AddVector(id: string, vector: []float32): error
        +SearchSimilar(query: []float32, topK: int): []SearchResult
        +GetVector(id: string): []float32
    }

    ControlPlane --> EmbeddingService
    ControlPlane --> ClusteringEngine
    ControlPlane --> PolicyEngine
    ControlPlane --> VectorDB
    EmbeddingService --> BGEModel
    ClusteringEngine --> Cluster
    ClusteringEngine --> VectorDB
    PolicyEngine --> Policy
```

### æ³³é“å›¾è®¾è®¡

#### é”™è¯¯å¤„ç†å’Œç­–ç•¥ç”Ÿæˆæµç¨‹

```mermaid
graph TD
    subgraph "å®¢æˆ·ç«¯"
        A[å‘é€è¯·æ±‚]
    end

    subgraph "ç½‘å…³ (æ•°æ®é¢)"
        B[æ¥æ”¶è¯·æ±‚]
        C[è·¯ç”±/é‰´æƒ]
        D{é™æµæ£€æŸ¥}
        E{ç†”æ–­æ£€æŸ¥}
        F[è½¬å‘è¯·æ±‚]
        G[æ¥æ”¶å“åº”]
        H{æ˜¯å¦å¼‚å¸¸}
        I[é”™è¯¯é‡‡æ ·]
        J[è„±æ•å¤„ç†]
        K[å‘é€åˆ°Kafka]
        L[è¿”å›å“åº”]
    end

    subgraph "æ¶ˆæ¯é˜Ÿåˆ—"
        M[Kafkaå­˜å‚¨]
    end

    subgraph "æ§åˆ¶é¢"
        N[æ¶ˆè´¹é”™è¯¯äº‹ä»¶]
        O[æ–‡æœ¬é¢„å¤„ç†]
        P[å‘é‡åŒ–]
        Q[ç›¸ä¼¼åº¦è®¡ç®—]
        R{æ‰¾åˆ°ç›¸ä¼¼ç°‡?}
        S[åŠ å…¥ç°æœ‰ç°‡]
        T[åˆ›å»ºæ–°ç°‡]
        U[æ›´æ–°ç°‡ç»Ÿè®¡]
        V[ç­–ç•¥è¯„ä¼°]
        W{è§¦å‘æ¡ä»¶?}
        X[ç”Ÿæˆç­–ç•¥]
        Y[ç­–ç•¥ä¸‹å‘]
    end

    subgraph "é…ç½®ä¸­å¿ƒ"
        Z[ETCDå­˜å‚¨ç­–ç•¥]
        AA[é€šçŸ¥ç½‘å…³æ›´æ–°]
    end

    A --> B
    B --> C
    C --> D
    D -->|é€šè¿‡| E
    D -->|é™æµ| L
    E -->|é€šè¿‡| F
    E -->|ç†”æ–­| L
    F --> G
    G --> H
    H -->|æ­£å¸¸| L
    H -->|å¼‚å¸¸| I
    I --> J
    J --> K
    K --> M
    M --> N
    N --> O
    O --> P
    P --> Q
    Q --> R
    R -->|æ˜¯| S
    R -->|å¦| T
    S --> U
    T --> U
    U --> V
    V --> W
    W -->|æ˜¯| X
    W -->|å¦| V
    X --> Y
    Y --> Z
    Z --> AA
    AA --> D
    AA --> E
```

### E-Rå›¾è®¾è®¡

#### æ•°æ®åº“è®¾è®¡

```mermaid
erDiagram
    CLUSTERS {
        string cluster_id PK
        text centroid_vector
        int error_count
        float severity
        timestamp create_time
        timestamp update_time
        text description
    }

    ERROR_EVENTS {
        string event_id PK
        string trace_id
        string span_id
        string request_path
        string method
        string service_name
        int status_code
        text error_message
        text stack_trace
        timestamp timestamp
        string cluster_id FK
    }

    VECTORS {
        string vector_id PK
        text vector_data
        string event_id FK
        timestamp created_at
        timestamp updated_at
    }

    POLICIES {
        string policy_id PK
        string cluster_id FK
        string policy_type
        float severity
        text policy_config
        timestamp create_time
        timestamp expire_time
        boolean is_active
    }

    CLUSTER_MEMBERS {
        string cluster_id FK
        string event_id FK
        float similarity_score
        timestamp joined_at
    }

    METRICS {
        string metric_id PK
        string cluster_id FK
        string metric_name
        float metric_value
        timestamp timestamp
        text labels
    }

    CLUSTERS ||--o{ ERROR_EVENTS : contains
    ERROR_EVENTS ||--|| VECTORS : has
    CLUSTERS ||--o{ POLICIES : triggers
    CLUSTERS ||--o{ CLUSTER_MEMBERS : includes
    ERROR_EVENTS ||--o{ CLUSTER_MEMBERS : belongs_to
    CLUSTERS ||--o{ METRICS : generates
```

### çŠ¶æ€æœºè®¾è®¡

#### ç†”æ–­å™¨çŠ¶æ€è½¬æ¢å›¾

```mermaid
stateDiagram-v2
    [*] --> CLOSED

    CLOSED --> OPEN : å¤±è´¥æ¬¡æ•° >= é˜ˆå€¼
    OPEN --> HALF_OPEN : è¶…æ—¶æ¢å¤
    HALF_OPEN --> CLOSED : æˆåŠŸæ¬¡æ•° >= é˜ˆå€¼
    HALF_OPEN --> OPEN : å¤±è´¥æ¬¡æ•° >= é˜ˆå€¼

    state CLOSED {
        [*] --> Monitoring
        Monitoring --> Recording_Success : æˆåŠŸè¯·æ±‚
        Monitoring --> Recording_Failure : å¤±è´¥è¯·æ±‚
        Recording_Success --> Monitoring
        Recording_Failure --> Monitoring
        Recording_Failure --> [*] : å¤±è´¥è®¡æ•°è¾¾åˆ°é˜ˆå€¼
    }

    state OPEN {
        [*] --> Rejecting_Requests
        Rejecting_Requests --> Waiting_Timeout : å¼€å§‹ç­‰å¾…
        Waiting_Timeout --> [*] : è¶…æ—¶
    }

    state HALF_OPEN {
        [*] --> Limited_Testing
        Limited_Testing --> Success_Count : æˆåŠŸè¯·æ±‚
        Limited_Testing --> Failure_Count : å¤±è´¥è¯·æ±‚
        Success_Count --> [*] : æˆåŠŸæ¬¡æ•°è¶³å¤Ÿ
        Failure_Count --> [*] : å¤±è´¥æ¬¡æ•°è¾¾åˆ°é˜ˆå€¼
    }
```

#### ç­–ç•¥ç”Ÿå‘½å‘¨æœŸçŠ¶æ€å›¾

```mermaid
stateDiagram-v2
    [*] --> EVALUATING

    EVALUATING --> TRIGGERED : è§¦å‘æ¡ä»¶æ»¡è¶³
    EVALUATING --> MONITORING : æœªè§¦å‘

    TRIGGERED --> POLICY_GENERATED : ç”Ÿæˆç­–ç•¥
    POLICY_GENERATED --> POLICY_APPLIED : ç­–ç•¥ä¸‹å‘
    POLICY_APPLIED --> ACTIVE : ç­–ç•¥ç”Ÿæ•ˆ

    ACTIVE --> RECOVERING : å¼‚å¸¸æ¶ˆé€€
    ACTIVE --> EXPIRED : ç­–ç•¥è¿‡æœŸ
    ACTIVE --> ESCALATING : å¼‚å¸¸åŠ é‡

    RECOVERING --> MONITORING : å®Œå…¨æ¢å¤
    ESCALATING --> POLICY_UPDATED : æ›´æ–°ç­–ç•¥
    POLICY_UPDATED --> ACTIVE
    EXPIRED --> MONITORING

    MONITORING --> EVALUATING : ç»§ç»­ç›‘æ§

    state ACTIVE {
        [*] --> Rate_Limiting
        [*] --> Circuit_Breaking
        [*] --> Degrading

        Rate_Limiting --> Adjusting_Rate : åŠ¨æ€è°ƒæ•´
        Circuit_Breaking --> Half_Open_Testing : åŠå¼€æµ‹è¯•
        Degrading --> Fallback_Response : é™çº§å“åº”

        Adjusting_Rate --> Rate_Limiting
        Half_Open_Testing --> Circuit_Breaking
        Fallback_Response --> Degrading
    }
```

### éƒ¨ç½²æ¶æ„å›¾

#### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ‹“æ‰‘

```mermaid
graph TB
    subgraph "è´Ÿè½½å‡è¡¡å±‚"
        LB[è´Ÿè½½å‡è¡¡å™¨ Nginx/Envoy]
    end

    subgraph "Kubernetesé›†ç¾¤"
        subgraph "ç½‘å…³æœåŠ¡ (æ•°æ®é¢)"
            GW1[Gateway Pod 1]
            GW2[Gateway Pod 2]
            GW3[Gateway Pod 3]
        end

        subgraph "æ§åˆ¶é¢æœåŠ¡"
            CP1[Control Plane Pod 1]
            CP2[Control Plane Pod 2]
        end

        subgraph "ä¸­é—´ä»¶æœåŠ¡"
            ETCD_CLUSTER[ETCDé›†ç¾¤]
            KAFKA_CLUSTER[Kafkaé›†ç¾¤]
        end
    end

    subgraph "å­˜å‚¨å±‚"
        PG_MASTER[(PostgreSQLä¸»)]
        PG_SLAVE[(PostgreSQLä»)]
        REDIS_CLUSTER[(Redisé›†ç¾¤)]
    end

    subgraph "ç›‘æ§ç³»ç»Ÿ"
        PROM[Prometheus]
        GRAF[Grafana]
        ALERT[AlertManager]
    end

    subgraph "ä¸‹æ¸¸æœåŠ¡"
        SVC1[Service A]
        SVC2[Service B]
        SVC3[Service C]
    end

    LB --> GW1
    LB --> GW2
    LB --> GW3

    GW1 --> SVC1
    GW1 --> SVC2
    GW1 --> SVC3
    GW2 --> SVC1
    GW2 --> SVC2
    GW2 --> SVC3
    GW3 --> SVC1
    GW3 --> SVC2
    GW3 --> SVC3

    GW1 --> KAFKA_CLUSTER
    GW2 --> KAFKA_CLUSTER
    GW3 --> KAFKA_CLUSTER

    GW1 --> ETCD_CLUSTER
    GW2 --> ETCD_CLUSTER
    GW3 --> ETCD_CLUSTER

    KAFKA_CLUSTER --> CP1
    KAFKA_CLUSTER --> CP2

    CP1 --> ETCD_CLUSTER
    CP2 --> ETCD_CLUSTER

    CP1 --> PG_MASTER
    CP2 --> PG_MASTER

    CP1 --> REDIS_CLUSTER
    CP2 --> REDIS_CLUSTER

    PG_MASTER --> PG_SLAVE

    GW1 --> PROM
    GW2 --> PROM
    GW3 --> PROM
    CP1 --> PROM
    CP2 --> PROM

    PROM --> GRAF
    PROM --> ALERT
```

---

## ğŸ”§ æ ¸å¿ƒæ¨¡å—æŠ€æœ¯å®ç°

### 1. æ•°æ®é¢ (Gateway) æŠ€æœ¯å®ç°

#### 1.1 ç½‘å…³æ ¸å¿ƒæ¡†æ¶

```go
// gateway/main.go
type Gateway struct {
    router         *gin.Engine
    rateLimiter    *RateLimiter
    circuitBreaker *CircuitBreaker
    errorSampler   *ErrorSampler
    vectorAgent    *VectorAgent
    configWatcher  *ConfigWatcher
}

// ä¸­é—´ä»¶é“¾è·¯
func (g *Gateway) setupMiddleware() {
    g.router.Use(
        middleware.Recovery(),
        middleware.Logger(),
        middleware.Tracing(),           // OpenTelemetryè¿½è¸ª
        middleware.Authentication(),    // JWT/OIDCè®¤è¯
        middleware.RateLimit(),        // åŸºäºç°‡çš„é™æµ
        middleware.CircuitBreaker(),   // åŸºäºç°‡çš„ç†”æ–­
        middleware.ErrorSampling(),    // é”™è¯¯é‡‡æ ·
        middleware.Metrics(),          // æŒ‡æ ‡æ”¶é›†
    )
}
```

#### 1.2 é”™è¯¯é‡‡æ ·å™¨å®ç°

```go
// gateway/sampler/error_sampler.go
type ErrorSampler struct {
    samplingRate float64
    kafkaProducer *kafka.Producer
    desensitizer *Desensitizer
}

type ErrorEvent struct {
    TraceID      string    `json:"trace_id"`
    SpanID       string    `json:"span_id"`
    RequestPath  string    `json:"request_path"`
    Method       string    `json:"method"`
    ServiceName  string    `json:"service_name"`
    StatusCode   int       `json:"status_code"`
    ErrorMessage string    `json:"error_message"`
    StackTrace   []string  `json:"stack_trace"`
    Timestamp    time.Time `json:"timestamp"`
}

func (es *ErrorSampler) SampleError(ctx *gin.Context, err error) {
    // é‡‡æ ·åˆ¤æ–­
    if rand.Float64() > es.samplingRate {
        return
    }

    // é”™è¯¯ä¿¡æ¯æå–
    event := &ErrorEvent{
        TraceID:      extractTraceID(ctx),
        SpanID:       extractSpanID(ctx),
        RequestPath:  ctx.Request.URL.Path,
        Method:       ctx.Request.Method,
        ServiceName:  extractServiceName(ctx),
        StatusCode:   ctx.Writer.Status(),
        ErrorMessage: err.Error(),
        StackTrace:   extractStackTrace(err, 2), // æå–å‰2å¸§
        Timestamp:    time.Now(),
    }

    // æ•æ„Ÿä¿¡æ¯è„±æ•
    event.ErrorMessage = es.desensitizer.Desensitize(event.ErrorMessage)

    // å¼‚æ­¥å‘é€åˆ°Kafka
    go es.sendToKafka(event)
}
```

#### 1.3 åŸºäºç°‡çš„é™æµå™¨å®ç°

```go
// gateway/limiter/cluster_rate_limiter.go
type ClusterRateLimiter struct {
    clusters map[string]*ClusterLimiter
    mutex    sync.RWMutex
    config   *ConfigWatcher
}

type ClusterLimiter struct {
    ClusterID    string
    TokenBucket  *TokenBucket
    Severity     float64  // ç°‡ä¸¥é‡åº¦ 0.0-1.0
    BaseRate     float64  // åŸºç¡€ä»¤ç‰Œé€Ÿç‡
    CurrentRate  float64  // å½“å‰ä»¤ç‰Œé€Ÿç‡
}

func (crl *ClusterRateLimiter) Allow(ctx *gin.Context) bool {
    clusterID := crl.identifyCluster(ctx)
    if clusterID == "" {
        return true // æ— æ³•è¯†åˆ«ç°‡ï¼Œæ”¾è¡Œ
    }

    crl.mutex.RLock()
    limiter, exists := crl.clusters[clusterID]
    crl.mutex.RUnlock()

    if !exists {
        return true // ç°‡ä¸å­˜åœ¨é™æµç­–ç•¥ï¼Œæ”¾è¡Œ
    }

    // åŸºäºç°‡ä¸¥é‡åº¦è°ƒæ•´ä»¤ç‰Œé€Ÿç‡
    adjustedRate := limiter.BaseRate * (1.0 - limiter.Severity)
    limiter.TokenBucket.SetRate(adjustedRate)

    return limiter.TokenBucket.Allow()
}

// é€šè¿‡å‘é‡ç›¸ä¼¼åº¦è¯†åˆ«è¯·æ±‚æ‰€å±ç°‡
func (crl *ClusterRateLimiter) identifyCluster(ctx *gin.Context) string {
    // ä»ä¸Šä¸‹æ–‡æå–é”™è¯¯ç‰¹å¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    errorSignature := extractErrorSignature(ctx)
    if errorSignature == "" {
        return ""
    }

    // è°ƒç”¨å‘é‡åŒ–Agentè®¡ç®—ç°‡å½’å±
    return crl.vectorAgent.IdentifyCluster(errorSignature)
}
```

#### 1.4 åŸºäºç°‡çš„ç†”æ–­å™¨å®ç°

```go
// gateway/breaker/cluster_circuit_breaker.go
type ClusterCircuitBreaker struct {
    clusters map[string]*ClusterBreaker
    mutex    sync.RWMutex
}

type ClusterBreaker struct {
    ClusterID     string
    State         BreakerState // CLOSED, OPEN, HALF_OPEN
    FailureCount  int64
    SuccessCount  int64
    LastFailTime  time.Time
    NextRetry     time.Time
    Config        *BreakerConfig
}

type BreakerConfig struct {
    FailureThreshold  int64         // å¤±è´¥æ¬¡æ•°é˜ˆå€¼
    RecoveryTimeout   time.Duration // æ¢å¤è¶…æ—¶æ—¶é—´
    RecoveryIncrement float64       // æ¢å¤å¢é‡ (20%)
}

func (ccb *ClusterCircuitBreaker) Allow(ctx *gin.Context, clusterID string) bool {
    ccb.mutex.Lock()
    breaker, exists := ccb.clusters[clusterID]
    if !exists {
        breaker = &ClusterBreaker{
            ClusterID: clusterID,
            State:     CLOSED,
            Config:    ccb.defaultConfig,
        }
        ccb.clusters[clusterID] = breaker
    }
    ccb.mutex.Unlock()

    switch breaker.State {
    case CLOSED:
        return true
    case OPEN:
        if time.Now().After(breaker.NextRetry) {
            breaker.State = HALF_OPEN
            return true
        }
        return false
    case HALF_OPEN:
        return true
    }

    return false
}
```

### 2. æ§åˆ¶é¢æŠ€æœ¯å®ç°

#### 2.1 å‘é‡åŒ–æœåŠ¡

```go
// control-plane/embedding/embedding_service.go
type EmbeddingService struct {
    model     *BGEModel
    cache     *lru.Cache
    batchSize int
}

type BGEModel struct {
    modelPath string
    session   *onnxruntime.Session
}

func (es *EmbeddingService) EmbedText(text string) ([]float32, error) {
    // æ£€æŸ¥ç¼“å­˜
    if cached, exists := es.cache.Get(text); exists {
        return cached.([]float32), nil
    }

    // æ–‡æœ¬é¢„å¤„ç†
    processedText := es.preprocessText(text)

    // è°ƒç”¨BGEæ¨¡å‹ç”Ÿæˆå‘é‡
    vector, err := es.model.Encode(processedText)
    if err != nil {
        return nil, err
    }

    // ç¼“å­˜ç»“æœ
    es.cache.Add(text, vector)

    return vector, nil
}

func (es *EmbeddingService) preprocessText(text string) string {
    // æ¨¡æ¿åŒ–å¤„ç†ï¼šå°†å˜é‡æ›¿æ¢ä¸ºå ä½ç¬¦
    text = regexp.MustCompile(`\b\d{11}\b`).ReplaceAllString(text, "[PHONE]")
    text = regexp.MustCompile(`\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b`).ReplaceAllString(text, "[EMAIL]")
    text = regexp.MustCompile(`\b[A-Za-z0-9]{20,}\b`).ReplaceAllString(text, "[TOKEN]")

    return text
}

// æ‰¹é‡å‘é‡åŒ–å¤„ç†
func (es *EmbeddingService) EmbedBatch(texts []string) ([][]float32, error) {
    vectors := make([][]float32, len(texts))

    // åˆ†æ‰¹å¤„ç†
    for i := 0; i < len(texts); i += es.batchSize {
        end := i + es.batchSize
        if end > len(texts) {
            end = len(texts)
        }

        batch := texts[i:end]
        batchVectors, err := es.model.EncodeBatch(batch)
        if err != nil {
            return nil, err
        }

        copy(vectors[i:end], batchVectors)
    }

    return vectors, nil
}
```

#### 2.2 èšç±»å¼•æ“å®ç°

```go
// control-plane/clustering/clustering_engine.go
type ClusteringEngine struct {
    vectorDB      *VectorDB
    clusterIndex  *faiss.Index
    clusters      map[string]*Cluster
    similarityThreshold float64 // 0.82
    mutex         sync.RWMutex
}

type Cluster struct {
    ID          string
    Centroid    []float32
    Members     []string
    ErrorCount  int64
    CreateTime  time.Time
    UpdateTime  time.Time
    Severity    float64
}

func (ce *ClusteringEngine) ProcessErrorEvent(event *ErrorEvent) error {
    // ç”Ÿæˆé”™è¯¯ç‰¹å¾æ–‡æœ¬
    featureText := ce.generateFeatureText(event)

    // å‘é‡åŒ–
    vector, err := ce.embeddingService.EmbedText(featureText)
    if err != nil {
        return err
    }

    // æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„ç°‡
    clusterID, similarity := ce.findMostSimilarCluster(vector)

    if similarity >= ce.similarityThreshold {
        // å½’å…¥ç°æœ‰ç°‡
        ce.addToCluster(clusterID, event, vector)
    } else {
        // åˆ›å»ºæ–°ç°‡
        newClusterID := ce.createNewCluster(event, vector)
        log.Printf("åˆ›å»ºæ–°å¼‚å¸¸ç°‡: %s", newClusterID)
    }

    return nil
}

func (ce *ClusteringEngine) findMostSimilarCluster(vector []float32) (string, float64) {
    ce.mutex.RLock()
    defer ce.mutex.RUnlock()

    var bestClusterID string
    var bestSimilarity float64

    for clusterID, cluster := range ce.clusters {
        similarity := ce.cosineSimilarity(vector, cluster.Centroid)
        if similarity > bestSimilarity {
            bestSimilarity = similarity
            bestClusterID = clusterID
        }
    }

    return bestClusterID, bestSimilarity
}

func (ce *ClusteringEngine) cosineSimilarity(a, b []float32) float64 {
    var dotProduct, normA, normB float64

    for i := range a {
        dotProduct += float64(a[i]) * float64(b[i])
        normA += float64(a[i]) * float64(a[i])
        normB += float64(b[i]) * float64(b[i])
    }

    return dotProduct / (math.Sqrt(normA) * math.Sqrt(normB))
}

// å®šæœŸé‡èšç±»ä¼˜åŒ–
func (ce *ClusteringEngine) ReCluster() {
    ce.mutex.Lock()
    defer ce.mutex.Unlock()

    // æ”¶é›†æ‰€æœ‰å‘é‡
    var vectors [][]float32
    var memberIDs []string

    for _, cluster := range ce.clusters {
        for _, memberID := range cluster.Members {
            vector, err := ce.vectorDB.GetVector(memberID)
            if err != nil {
                continue
            }
            vectors = append(vectors, vector)
            memberIDs = append(memberIDs, memberID)
        }
    }

    // ä½¿ç”¨DBSCANé‡æ–°èšç±»
    newClusters := ce.dbscanClustering(vectors, memberIDs)

    // æ›´æ–°ç°‡ä¿¡æ¯
    ce.clusters = newClusters

    log.Printf("é‡èšç±»å®Œæˆï¼Œå½“å‰ç°‡æ•°é‡: %d", len(ce.clusters))
}
```

#### 2.3 ç­–ç•¥å¼•æ“å®ç°

```go
// control-plane/policy/policy_engine.go
type PolicyEngine struct {
    clusters      map[string]*Cluster
    policies      map[string]*Policy
    configStore   *ConfigStore
    metrics       *MetricsCollector
}

type Policy struct {
    ClusterID     string
    PolicyType    PolicyType // RATE_LIMIT, CIRCUIT_BREAK, DEGRADE
    Severity      float64
    RateLimit     *RateLimitPolicy
    CircuitBreak  *CircuitBreakPolicy
    CreateTime    time.Time
    ExpireTime    time.Time
}

type RateLimitPolicy struct {
    LimitRate    float64  // é™åˆ¶æ¯”ä¾‹ 0.0-1.0
    Duration     time.Duration
}

type CircuitBreakPolicy struct {
    BreakDuration time.Duration
    RecoveryStep  float64 // æ¢å¤æ­¥é•¿
}

func (pe *PolicyEngine) EvaluatePolicies() {
    for clusterID, cluster := range pe.clusters {
        // è®¡ç®—æ—¶é—´çª—å£å†…çš„é”™è¯¯æŒ‡æ ‡
        windowSize := 10 * time.Second
        errorRate := pe.calculateErrorRate(clusterID, windowSize)
        growthRate := pe.calculateGrowthRate(clusterID, windowSize)

        // åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘ç­–ç•¥
        shouldTrigger := pe.shouldTriggerPolicy(errorRate, growthRate)

        if shouldTrigger {
            policy := pe.generatePolicy(cluster, errorRate, growthRate)
            pe.applyPolicy(policy)

            log.Printf("ä¸ºç°‡ %s ç”Ÿæˆç­–ç•¥: ç±»å‹=%v, ä¸¥é‡åº¦=%.2f",
                clusterID, policy.PolicyType, policy.Severity)
        }
    }
}

func (pe *PolicyEngine) shouldTriggerPolicy(errorRate, growthRate float64) bool {
    // PRDä¸­å®šä¹‰çš„è§¦å‘æ¡ä»¶
    errorThreshold := 0.20  // 20%
    growthThreshold := 50.0 // 50æ¬¡/10s

    return errorRate >= errorThreshold && growthRate >= growthThreshold
}

func (pe *PolicyEngine) generatePolicy(cluster *Cluster, errorRate, growthRate float64) *Policy {
    // æ ¹æ®é”™è¯¯ç‡å’Œå¢é•¿ç‡è®¡ç®—ä¸¥é‡åº¦
    severity := pe.calculateSeverity(errorRate, growthRate)

    var policyType PolicyType
    if severity >= 0.8 {
        policyType = CIRCUIT_BREAK
    } else if severity >= 0.4 {
        policyType = RATE_LIMIT
    } else {
        policyType = DEGRADE
    }

    policy := &Policy{
        ClusterID:  cluster.ID,
        PolicyType: policyType,
        Severity:   severity,
        CreateTime: time.Now(),
        ExpireTime: time.Now().Add(5 * time.Minute),
    }

    switch policyType {
    case RATE_LIMIT:
        policy.RateLimit = &RateLimitPolicy{
            LimitRate: severity * 0.8, // æœ€å¤§é™åˆ¶80%
            Duration:  time.Minute,
        }
    case CIRCUIT_BREAK:
        policy.CircuitBreak = &CircuitBreakPolicy{
            BreakDuration: 30 * time.Second,
            RecoveryStep:  0.2, // æ¯æ¬¡æ¢å¤20%
        }
    }

    return policy
}

// ç­–ç•¥ä¸‹å‘åˆ°æ•°æ®é¢
func (pe *PolicyEngine) applyPolicy(policy *Policy) {
    // å°†ç­–ç•¥å†™å…¥ETCD
    key := fmt.Sprintf("/policies/%s", policy.ClusterID)
    value, _ := json.Marshal(policy)

    pe.configStore.Put(key, string(value))

    // è®°å½•æŒ‡æ ‡
    pe.metrics.PolicyApplied.WithLabelValues(
        policy.ClusterID,
        string(policy.PolicyType),
    ).Inc()
}
```

### 3. å‘é‡æ•°æ®åº“è®¾è®¡

```go
// control-plane/vectordb/vector_db.go
type VectorDB struct {
    faissIndex *faiss.Index
    pgConn     *sql.DB
    cache      *lru.Cache
}

func NewVectorDB() *VectorDB {
    // åˆ›å»ºFAISSç´¢å¼• (å†…å­˜)
    dimension := 768 // BGE-smallå‘é‡ç»´åº¦
    index := faiss.NewIndexFlatIP(dimension)

    // PostgreSQLè¿æ¥ (æŒä¹…åŒ–)
    pgConn, _ := sql.Open("postgres", "postgres://user:pass@localhost/vectordb")

    return &VectorDB{
        faissIndex: index,
        pgConn:     pgConn,
        cache:      lru.New(10000),
    }
}

func (vdb *VectorDB) AddVector(id string, vector []float32) error {
    // æ·»åŠ åˆ°FAISSç´¢å¼•
    vdb.faissIndex.Add([][]float32{vector})

    // æŒä¹…åŒ–åˆ°PostgreSQL
    _, err := vdb.pgConn.Exec(`
        INSERT INTO vectors (id, vector, created_at)
        VALUES ($1, $2, NOW())
        ON CONFLICT (id) DO UPDATE SET
        vector = $2, updated_at = NOW()
    `, id, pq.Array(vector))

    return err
}

func (vdb *VectorDB) SearchSimilar(query []float32, topK int) ([]SearchResult, error) {
    // ä½¿ç”¨FAISSè¿›è¡Œç›¸ä¼¼åº¦æœç´¢
    distances, ids, err := vdb.faissIndex.Search([][]float32{query}, topK)
    if err != nil {
        return nil, err
    }

    results := make([]SearchResult, len(ids[0]))
    for i, id := range ids[0] {
        results[i] = SearchResult{
            ID:         fmt.Sprintf("%d", id),
            Similarity: float64(distances[0][i]),
        }
    }

    return results, nil
}
```

---

## ğŸ“Š æ•°æ®æµç¨‹è®¾è®¡

### é”™è¯¯å¤„ç†æµç¨‹

```
1. è¯·æ±‚å¼‚å¸¸ â†’ Gatewayé”™è¯¯å¤„ç†ä¸­é—´ä»¶
2. é”™è¯¯é‡‡æ · â†’ æ•æ„Ÿä¿¡æ¯è„±æ•
3. Kafkaå¼‚æ­¥å‘é€ â†’ æ§åˆ¶é¢æ¶ˆè´¹
4. æ–‡æœ¬å‘é‡åŒ– â†’ å‘é‡ç›¸ä¼¼åº¦è®¡ç®—
5. ç°‡å½’å±åˆ¤æ–­ â†’ æ›´æ–°ç°‡ç»Ÿè®¡ä¿¡æ¯
6. ç­–ç•¥è¯„ä¼° â†’ ç”Ÿæˆé™æµ/ç†”æ–­ç­–ç•¥
7. ETCDç­–ç•¥ä¸‹å‘ â†’ Gatewayé…ç½®æ›´æ–°
8. ç²¾ç¡®é™æµæ‰§è¡Œ â†’ ä¿æŠ¤ä¸‹æ¸¸æœåŠ¡
```

### é…ç½®æ›´æ–°æµç¨‹

```go
// gateway/config/config_watcher.go
type ConfigWatcher struct {
    etcdClient *etcd.Client
    policies   map[string]*Policy
    callbacks  []ConfigUpdateCallback
    mutex      sync.RWMutex
}

func (cw *ConfigWatcher) WatchPolicyUpdates() {
    watchChan := cw.etcdClient.Watch(context.Background(), "/policies/", etcd.WithPrefix())

    for watchResp := range watchChan {
        for _, event := range watchResp.Events {
            clusterID := strings.TrimPrefix(string(event.Kv.Key), "/policies/")

            switch event.Type {
            case etcd.EventTypePut:
                var policy Policy
                json.Unmarshal(event.Kv.Value, &policy)

                cw.mutex.Lock()
                cw.policies[clusterID] = &policy
                cw.mutex.Unlock()

                // é€šçŸ¥æ‰€æœ‰ç›‘å¬å™¨
                for _, callback := range cw.callbacks {
                    callback.OnPolicyUpdate(clusterID, &policy)
                }

            case etcd.EventTypeDelete:
                cw.mutex.Lock()
                delete(cw.policies, clusterID)
                cw.mutex.Unlock()

                for _, callback := range cw.callbacks {
                    callback.OnPolicyDelete(clusterID)
                }
            }
        }
    }
}
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

### 1. å‘é‡è®¡ç®—ä¼˜åŒ–

```go
// å‘é‡æ‰¹å¤„ç†ä¼˜åŒ–
type BatchProcessor struct {
    batchSize   int
    maxWaitTime time.Duration
    buffer      []*ErrorEvent
    mutex       sync.Mutex
}

func (bp *BatchProcessor) AddEvent(event *ErrorEvent) {
    bp.mutex.Lock()
    defer bp.mutex.Unlock()

    bp.buffer = append(bp.buffer, event)

    if len(bp.buffer) >= bp.batchSize {
        go bp.processBatch(bp.buffer[:bp.batchSize])
        bp.buffer = bp.buffer[bp.batchSize:]
    }
}

// SIMDä¼˜åŒ–çš„ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
func CosineSimilaritySIMD(a, b []float32) float64 {
    // ä½¿ç”¨AVXæŒ‡ä»¤é›†ä¼˜åŒ–å‘é‡è¿ç®—
    return simd.CosineSimilarity(a, b)
}
```

### 2. ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

```go
type MultiLevelCache struct {
    l1Cache *sync.Map        // çƒ­ç‚¹æ•°æ®
    l2Cache *lru.Cache       // LRUç¼“å­˜
    l3Cache *redis.Client    // Redisåˆ†å¸ƒå¼ç¼“å­˜
}

func (mlc *MultiLevelCache) Get(key string) (interface{}, bool) {
    // L1ç¼“å­˜æŸ¥æ‰¾
    if value, ok := mlc.l1Cache.Load(key); ok {
        return value, true
    }

    // L2ç¼“å­˜æŸ¥æ‰¾
    if value, ok := mlc.l2Cache.Get(key); ok {
        mlc.l1Cache.Store(key, value) // æå‡åˆ°L1
        return value, true
    }

    // L3ç¼“å­˜æŸ¥æ‰¾
    value, err := mlc.l3Cache.Get(key).Result()
    if err == nil {
        mlc.l2Cache.Add(key, value)    // æå‡åˆ°L2
        mlc.l1Cache.Store(key, value)  // æå‡åˆ°L1
        return value, true
    }

    return nil, false
}
```

### 3. ç´¢å¼•ä¼˜åŒ–

```go
// åˆ†å±‚ç´¢å¼•ç»“æ„
type HierarchicalIndex struct {
    coarseIndex *faiss.Index  // ç²—ç²’åº¦ç´¢å¼•
    fineIndex   *faiss.Index  // ç»†ç²’åº¦ç´¢å¼•
    threshold   float64
}

func (hi *HierarchicalIndex) Search(query []float32, topK int) ([]SearchResult, error) {
    // ç¬¬ä¸€é˜¶æ®µï¼šç²—ç²’åº¦æœç´¢
    coarseResults, err := hi.coarseIndex.Search([][]float32{query}, topK*2)
    if err != nil {
        return nil, err
    }

    // ç¬¬äºŒé˜¶æ®µï¼šç»†ç²’åº¦ç²¾ç¡®æœç´¢
    var candidates [][]float32
    for _, id := range coarseResults[0] {
        vector, _ := hi.getVector(int64(id))
        candidates = append(candidates, vector)
    }

    fineResults, err := hi.fineIndex.Search(candidates, topK)
    return hi.convertResults(fineResults), err
}
```

---

## ğŸ“ˆ ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### ç›‘æ§æŒ‡æ ‡è®¾è®¡

```go
// monitoring/metrics.go
var (
    // ç½‘å…³æŒ‡æ ‡
    RequestTotal = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "gateway_requests_total",
            Help: "Total number of requests",
        },
        []string{"method", "path", "status", "cluster_id"},
    )

    RequestDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "gateway_request_duration_seconds",
            Help:    "Request duration in seconds",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "path", "cluster_id"},
    )

    // é™æµæŒ‡æ ‡
    RateLimitHits = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "rate_limit_hits_total",
            Help: "Total number of rate limit hits",
        },
        []string{"cluster_id", "policy_type"},
    )

    // ç†”æ–­æŒ‡æ ‡
    CircuitBreakerState = prometheus.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "circuit_breaker_state",
            Help: "Circuit breaker state (0=closed, 1=open, 2=half-open)",
        },
        []string{"cluster_id"},
    )

    // èšç±»æŒ‡æ ‡
    ClusterSize = prometheus.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "cluster_size",
            Help: "Number of errors in cluster",
        },
        []string{"cluster_id"},
    )

    ClusterSeverity = prometheus.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "cluster_severity",
            Help: "Cluster severity score",
        },
        []string{"cluster_id"},
    )
)
```

### é“¾è·¯è¿½è¸ªå®ç°

```go
// tracing/tracer.go
func TracingMiddleware() gin.HandlerFunc {
    return gin.HandlerFunc(func(c *gin.Context) {
        tracer := otel.Tracer("llm-aware-gateway")

        ctx, span := tracer.Start(c.Request.Context(), "gateway.request")
        defer span.End()

        // æ·»åŠ è¿½è¸ªå±æ€§
        span.SetAttributes(
            attribute.String("http.method", c.Request.Method),
            attribute.String("http.path", c.Request.URL.Path),
            attribute.String("user.agent", c.Request.UserAgent()),
        )

        // æ³¨å…¥è¿½è¸ªä¸Šä¸‹æ–‡
        c.Request = c.Request.WithContext(ctx)

        c.Next()

        // è®°å½•å“åº”çŠ¶æ€
        span.SetAttributes(
            attribute.Int("http.status_code", c.Writer.Status()),
        )

        if c.Writer.Status() >= 400 {
            span.SetStatus(codes.Error, "Request failed")

            // å¦‚æœæ˜¯é”™è¯¯è¯·æ±‚ï¼Œæ·»åŠ ç°‡ä¿¡æ¯
            if clusterID := c.GetString("cluster_id"); clusterID != "" {
                span.SetAttributes(
                    attribute.String("cluster.id", clusterID),
                )
            }
        }
    })
}
```

---

## ğŸ› ï¸ éƒ¨ç½²æ–¹æ¡ˆ

### Dockerå®¹å™¨åŒ–

```dockerfile
# Dockerfile.gateway
FROM golang:1.21-alpine AS builder

WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download

COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -o gateway ./cmd/gateway

FROM alpine:latest
RUN apk --no-cache add ca-certificates tzdata
WORKDIR /root/

COPY --from=builder /app/gateway .
COPY --from=builder /app/configs ./configs

EXPOSE 8080
CMD ["./gateway", "--config", "configs/gateway.yaml"]
```

### Kuberneteséƒ¨ç½²

```yaml
# deploy/gateway-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-aware-gateway
  namespace: gateway
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-aware-gateway
  template:
    metadata:
      labels:
        app: llm-aware-gateway
    spec:
      containers:
      - name: gateway
        image: llm-aware-gateway:latest
        ports:
        - containerPort: 8080
        env:
        - name: ETCD_ENDPOINTS
          value: "etcd:2379"
        - name: KAFKA_BROKERS
          value: "kafka:9092"
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: gateway-service
  namespace: gateway
spec:
  selector:
    app: llm-aware-gateway
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
```

### Helm Chart

```yaml
# helm/values.yaml
gateway:
  replicas: 3
  image:
    repository: llm-aware-gateway
    tag: latest

  config:
    sampling_rate: 0.05
    similarity_threshold: 0.82
    error_rate_threshold: 0.20
    growth_rate_threshold: 50

  resources:
    requests:
      memory: "256Mi"
      cpu: "200m"
    limits:
      memory: "512Mi"
      cpu: "500m"

controlplane:
  replicas: 2
  image:
    repository: llm-aware-controlplane
    tag: latest

  embedding:
    model_path: "/models/bge-small-zh"
    batch_size: 50
    cache_size: 10000

  clustering:
    reclustering_interval: "5m"
    min_cluster_size: 10

etcd:
  enabled: true
  replicas: 3

kafka:
  enabled: true
  replicas: 3
  topics:
    - name: error-events
      partitions: 6
      replicas: 2

postgresql:
  enabled: true
  replicas: 1
  extensions:
    - vector
```

---

## ğŸ§ª æµ‹è¯•æ–¹æ¡ˆ

### å•å…ƒæµ‹è¯•

```go
// gateway/limiter/limiter_test.go
func TestClusterRateLimiter(t *testing.T) {
    limiter := NewClusterRateLimiter()

    // æµ‹è¯•åŸºç¡€é™æµ
    cluster := &ClusterLimiter{
        ClusterID:   "test-cluster",
        TokenBucket: NewTokenBucket(100), // 100 TPS
        Severity:    0.5,                 // 50% ä¸¥é‡åº¦
        BaseRate:    100,
    }
    limiter.clusters["test-cluster"] = cluster

    ctx := &gin.Context{}
    ctx.Set("cluster_id", "test-cluster")

    // å‰50ä¸ªè¯·æ±‚åº”è¯¥è¢«å…è®¸ (100 * (1-0.5) = 50 TPS)
    allowed := 0
    for i := 0; i < 100; i++ {
        if limiter.Allow(ctx) {
            allowed++
        }
    }

    assert.Equal(t, 50, allowed)
}
```

### é›†æˆæµ‹è¯•

```go
// test/integration_test.go
func TestEndToEndFlow(t *testing.T) {
    // å¯åŠ¨æµ‹è¯•ç¯å¢ƒ
    testEnv := setupTestEnvironment()
    defer testEnv.Cleanup()

    // 1. å‘é€æ­£å¸¸è¯·æ±‚
    resp := testEnv.SendRequest("GET", "/api/users", nil)
    assert.Equal(t, 200, resp.StatusCode)

    // 2. æ¨¡æ‹Ÿé”™è¯¯è¯·æ±‚
    for i := 0; i < 100; i++ {
        testEnv.SendRequest("GET", "/api/error-endpoint", nil)
    }

    // 3. ç­‰å¾…èšç±»å’Œç­–ç•¥ç”Ÿæˆ
    time.Sleep(30 * time.Second)

    // 4. éªŒè¯é™æµç­–ç•¥ç”Ÿæ•ˆ
    rateLimited := 0
    for i := 0; i < 50; i++ {
        resp := testEnv.SendRequest("GET", "/api/error-endpoint", nil)
        if resp.StatusCode == 429 {
            rateLimited++
        }
    }

    assert.Greater(t, rateLimited, 10, "åº”è¯¥æœ‰éƒ¨åˆ†è¯·æ±‚è¢«é™æµ")
}
```

### å‹åŠ›æµ‹è¯•

```bash
#!/bin/bash
# test/stress_test.sh

# ä½¿ç”¨wrkè¿›è¡Œå‹åŠ›æµ‹è¯•
wrk -t12 -c400 -d30s --script=lua/stress_test.lua http://gateway:8080/api/test

# ç›‘æ§å…³é”®æŒ‡æ ‡
while true; do
    echo "=== $(date) ==="
    curl -s http://gateway:8080/metrics | grep -E "(request_duration|rate_limit_hits|cluster_size)"
    sleep 10
done
```

---

## ğŸ”„ å‘å¸ƒä¸è¿ç»´

### ç°åº¦å‘å¸ƒç­–ç•¥

```yaml
# ä½¿ç”¨Istioè¿›è¡Œç°åº¦å‘å¸ƒ
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: gateway-canary
spec:
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: gateway-service
        subset: canary
      weight: 100
  - route:
    - destination:
        host: gateway-service
        subset: stable
      weight: 90
    - destination:
        host: gateway-service
        subset: canary
      weight: 10
```

### ç›‘æ§å‘Šè­¦

```yaml
# å…³é”®æŒ‡æ ‡å‘Šè­¦è§„åˆ™
groups:
- name: llm-aware-gateway
  rules:
  - alert: HighErrorRate
    expr: rate(gateway_requests_total{status=~"5.."}[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "ç½‘å…³é”™è¯¯ç‡è¿‡é«˜"
      description: "é”™è¯¯ç‡: {{ $value }}"

  - alert: ClusterSizeAnomaly
    expr: cluster_size > 1000
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "å¼‚å¸¸ç°‡è§„æ¨¡è¿‡å¤§"
      description: "ç°‡ {{ $labels.cluster_id }} è§„æ¨¡: {{ $value }}"

  - alert: PolicyEngineDown
    expr: up{job="control-plane"} == 0
    for: 30s
    labels:
      severity: critical
    annotations:
      summary: "æ§åˆ¶é¢æœåŠ¡ä¸‹çº¿"
```

### è‡ªåŠ¨æ‰©ç¼©å®¹

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gateway-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-aware-gateway
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
```

---

## ğŸ¯ æ€»ç»“

æœ¬æŠ€æœ¯æ–¹æ¡ˆè¯¦ç»†æè¿°äº†LLM-Aware Gatewayçš„å®Œæ•´å®ç°æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š

1. **åˆ†å±‚æ¶æ„è®¾è®¡**ï¼šæ•°æ®é¢è´Ÿè´£é«˜æ€§èƒ½æµé‡å¤„ç†ï¼Œæ§åˆ¶é¢è´Ÿè´£æ™ºèƒ½å†³ç­–
2. **æ ¸å¿ƒç®—æ³•å®ç°**ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦çš„èšç±»ç®—æ³•ï¼Œç²¾ç¡®è¯†åˆ«åŒç±»å¼‚å¸¸
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šå¤šçº§ç¼“å­˜ã€æ‰¹å¤„ç†ã€SIMDä¼˜åŒ–ç­‰æ‰‹æ®µä¿è¯ä½å»¶è¿Ÿ
4. **å¯è§‚æµ‹æ€§**ï¼šå®Œæ•´çš„ç›‘æ§ã€é“¾è·¯è¿½è¸ªã€å‘Šè­¦ä½“ç³»
5. **äº‘åŸç”Ÿéƒ¨ç½²**ï¼šåŸºäºKubernetesçš„å®¹å™¨åŒ–éƒ¨ç½²ï¼Œæ”¯æŒå¼¹æ€§ä¼¸ç¼©

è¯¥æ–¹æ¡ˆèƒ½å¤Ÿå®ç°PRDä¸­å®šä¹‰çš„æ ¸å¿ƒç›®æ ‡ï¼š
- å‡ç¾æ•ˆæœï¼šåŒæºæ•…éšœ5xxå³°å€¼ä¸‹é™â‰¥40%
- æ€§èƒ½å¼€é”€ï¼šP95å»¶è¿Ÿå¢åŠ â‰¤2ms
- è¯¯æ€ç‡ï¼šè¢«é™æµä½†å®é™…æˆåŠŸâ‰¤2%
- æ¢å¤é€Ÿåº¦ï¼šæ•…éšœæ¶ˆé™¤60så†…æ¢å¤

é€šè¿‡è¯­ä¹‰æ„ŸçŸ¥çš„æ™ºèƒ½ç†”æ–­é™æµï¼Œæ˜¾è‘—æå‡ç³»ç»Ÿç¨³å®šæ€§å’Œç”¨æˆ·ä½“éªŒã€‚
