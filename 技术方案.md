# LLM-Aware Gateway 技术方案

> 语义感知熔断/限流网关的详细技术实现方案

## 📋 技术方案概览

### 方案目标
构建一个基于语义分析的智能网关系统，通过大语言模型技术识别同类错误并实现精确熔断限流，相比传统基于QPS/错误率的粗粒度策略，显著降低误杀率并提升系统稳定性。

### 技术选型

| 组件类别 | 技术选型 | 说明 |
|---------|----------|------|
| 编程语言 | Go 1.21+ | 高性能、并发友好 |
| 网关框架 | Gin + 自研中间件 | 轻量级、可扩展 |
| 向量数据库 | FAISS + pgvector | FAISS内存计算，pgvector持久化 |
| 消息队列 | Kafka | 异步数据传输 |
| 配置中心 | ETCD | 策略配置热更新 |
| 监控系统 | Prometheus + Grafana | 指标监控 |
| 链路追踪 | OpenTelemetry | 分布式追踪 |
| 嵌入模型 | BGE-small-zh | 轻量中文向量化模型 |

---

## 🏗️ 系统架构设计

### 整体架构

```
┌─────────────────────────────────────────────────────────────┐
│                        客户端层                              │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│                   负载均衡层                                  │
│              Nginx/Envoy/CloudFlare                        │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│                    数据面 (Gateway)                         │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │
│  │   路由模块   │  │   鉴权模块   │  │    限流/熔断模块     │  │
│  └─────────────┘  └─────────────┘  └─────────────────────┘  │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │
│  │  错误采样器  │  │ 向量化Agent │  │     指标采集器       │  │
│  └─────────────┘  └─────────────┘  └─────────────────────┘  │
└─────────────────────┬───────────────────────────────────────┘
                      │ (Kafka消息队列)
┌─────────────────────▼───────────────────────────────────────┐
│                    控制面 (Control Plane)                   │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │
│  │  嵌入服务    │  │  向量数据库  │  │     聚类引擎         │  │
│  │ BGE-small   │  │FAISS+pgvec │  │   DBSCAN/HNSW       │  │
│  └─────────────┘  └─────────────┘  └─────────────────────┘  │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────┐  │
│  │  策略引擎    │  │  配置中心    │  │     监控中心         │  │
│  │ 规则生成     │  │   ETCD      │  │  Prometheus         │  │
│  └─────────────┘  └─────────────┘  └─────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

### 数据流转架构

```
请求流程：
Client → Gateway → 路由/鉴权 → 限流检查 → 熔断检查 → Downstream
              ↓
         错误采样 → Kafka → 控制面处理 → 策略生成 → ETCD → Gateway更新

向量化流程：
错误文本 → 文本预处理 → 嵌入模型 → 向量 → FAISS索引 → 聚类分析
```

### 系统交互设计

#### 服务间通信时序图

```mermaid
sequenceDiagram
    participant Client as 客户端
    participant Gateway as 网关(数据面)
    participant Kafka as 消息队列
    participant Control as 控制面
    participant ETCD as 配置中心
    participant Vector as 向量数据库
    participant Downstream as 下游服务

    Client->>Gateway: HTTP请求
    Gateway->>Gateway: 鉴权/路由检查
    Gateway->>Gateway: 限流检查
    Gateway->>Gateway: 熔断检查

    alt 请求正常
        Gateway->>Downstream: 转发请求
        Downstream-->>Gateway: 响应
        Gateway-->>Client: 返回响应
    else 请求异常
        Gateway->>Gateway: 错误采样
        Gateway->>Kafka: 异步发送错误事件
        Gateway-->>Client: 返回错误响应

        Kafka->>Control: 消费错误事件
        Control->>Control: 文本向量化
        Control->>Vector: 存储向量
        Control->>Control: 聚类分析
        Control->>Control: 策略评估
        Control->>ETCD: 下发策略配置
        ETCD->>Gateway: 配置更新通知
        Gateway->>Gateway: 更新限流/熔断策略
    end
```

#### 组件依赖关系图

```mermaid
graph TB
    subgraph "数据面 (Data Plane)"
        GW[网关服务]
        RL[限流器]
        CB[熔断器]
        ES[错误采样器]
        VA[向量Agent]
        CW[配置监听器]
    end

    subgraph "控制面 (Control Plane)"
        EM[嵌入服务]
        CE[聚类引擎]
        PE[策略引擎]
        MC[指标收集器]
    end

    subgraph "存储层"
        VDB[(向量数据库)]
        ETCD[(配置中心)]
        KAFKA[(消息队列)]
        PG[(PostgreSQL)]
        REDIS[(Redis缓存)]
    end

    subgraph "监控层"
        PROM[Prometheus]
        GRAF[Grafana]
        ALERT[AlertManager]
    end

    GW --> RL
    GW --> CB
    GW --> ES
    GW --> VA
    GW --> CW

    ES --> KAFKA
    KAFKA --> CE
    CE --> EM
    CE --> VDB
    CE --> PE
    PE --> ETCD
    ETCD --> CW

    VDB --> PG
    VDB --> REDIS
    EM --> REDIS

    GW --> PROM
    CE --> PROM
    PE --> PROM
    PROM --> GRAF
    PROM --> ALERT
```

---

## 📐 详细设计

### 类图设计

#### 1. 网关核心类图

```mermaid
classDiagram
    class Gateway {
        -router: gin.Engine
        -rateLimiter: RateLimiter
        -circuitBreaker: CircuitBreaker
        -errorSampler: ErrorSampler
        -vectorAgent: VectorAgent
        -configWatcher: ConfigWatcher
        +setupMiddleware(): void
        +start(): error
        +stop(): error
    }

    class RateLimiter {
        -clusters: map[string]*ClusterLimiter
        -mutex: sync.RWMutex
        +Allow(ctx: gin.Context): bool
        +updatePolicy(clusterID: string, policy: Policy): void
    }

    class ClusterLimiter {
        -clusterID: string
        -tokenBucket: TokenBucket
        -severity: float64
        -baseRate: float64
        -currentRate: float64
        +allow(): bool
        +updateSeverity(severity: float64): void
    }

    class TokenBucket {
        -capacity: int64
        -tokens: int64
        -refillRate: float64
        -lastRefill: time.Time
        +allow(): bool
        +setRate(rate: float64): void
    }

    class CircuitBreaker {
        -clusters: map[string]*ClusterBreaker
        -mutex: sync.RWMutex
        +Allow(ctx: gin.Context, clusterID: string): bool
        +RecordSuccess(clusterID: string): void
        +RecordFailure(clusterID: string): void
    }

    class ClusterBreaker {
        -clusterID: string
        -state: BreakerState
        -failureCount: int64
        -successCount: int64
        -lastFailTime: time.Time
        -nextRetry: time.Time
        -config: BreakerConfig
        +allow(): bool
        +recordResult(success: bool): void
    }

    class ErrorSampler {
        -samplingRate: float64
        -kafkaProducer: kafka.Producer
        -desensitizer: Desensitizer
        +SampleError(ctx: gin.Context, err: error): void
        +sendToKafka(event: ErrorEvent): error
    }

    class VectorAgent {
        -embeddingService: EmbeddingService
        -cache: lru.Cache
        +IdentifyCluster(errorSignature: string): string
        +generateVector(text: string): []float32
    }

    Gateway --> RateLimiter
    Gateway --> CircuitBreaker
    Gateway --> ErrorSampler
    Gateway --> VectorAgent
    RateLimiter --> ClusterLimiter
    ClusterLimiter --> TokenBucket
    CircuitBreaker --> ClusterBreaker
```

#### 2. 控制面核心类图

```mermaid
classDiagram
    class ControlPlane {
        -embeddingService: EmbeddingService
        -clusteringEngine: ClusteringEngine
        -policyEngine: PolicyEngine
        -vectorDB: VectorDB
        -configStore: ConfigStore
        +start(): error
        +processErrorEvent(event: ErrorEvent): error
    }

    class EmbeddingService {
        -model: BGEModel
        -cache: lru.Cache
        -batchSize: int
        +EmbedText(text: string): []float32
        +EmbedBatch(texts: []string): [][]float32
        +preprocessText(text: string): string
    }

    class BGEModel {
        -modelPath: string
        -session: onnxruntime.Session
        +Encode(text: string): []float32
        +EncodeBatch(texts: []string): [][]float32
    }

    class ClusteringEngine {
        -vectorDB: VectorDB
        -clusterIndex: faiss.Index
        -clusters: map[string]*Cluster
        -similarityThreshold: float64
        -mutex: sync.RWMutex
        +ProcessErrorEvent(event: ErrorEvent): error
        +findMostSimilarCluster(vector: []float32): (string, float64)
        +createNewCluster(event: ErrorEvent, vector: []float32): string
        +ReCluster(): void
    }

    class Cluster {
        -id: string
        -centroid: []float32
        -members: []string
        -errorCount: int64
        -createTime: time.Time
        -updateTime: time.Time
        -severity: float64
        +addMember(memberID: string): void
        +updateCentroid(vectors: [][]float32): void
        +calculateSeverity(): float64
    }

    class PolicyEngine {
        -clusters: map[string]*Cluster
        -policies: map[string]*Policy
        -configStore: ConfigStore
        -metrics: MetricsCollector
        +EvaluatePolicies(): void
        +generatePolicy(cluster: Cluster): Policy
        +applyPolicy(policy: Policy): void
        +shouldTriggerPolicy(errorRate, growthRate: float64): bool
    }

    class Policy {
        -clusterID: string
        -policyType: PolicyType
        -severity: float64
        -rateLimit: RateLimitPolicy
        -circuitBreak: CircuitBreakPolicy
        -createTime: time.Time
        -expireTime: time.Time
    }

    class VectorDB {
        -faissIndex: faiss.Index
        -pgConn: sql.DB
        -cache: lru.Cache
        +AddVector(id: string, vector: []float32): error
        +SearchSimilar(query: []float32, topK: int): []SearchResult
        +GetVector(id: string): []float32
    }

    ControlPlane --> EmbeddingService
    ControlPlane --> ClusteringEngine
    ControlPlane --> PolicyEngine
    ControlPlane --> VectorDB
    EmbeddingService --> BGEModel
    ClusteringEngine --> Cluster
    ClusteringEngine --> VectorDB
    PolicyEngine --> Policy
```

### 泳道图设计

#### 错误处理和策略生成流程

```mermaid
graph TD
    subgraph "客户端"
        A[发送请求]
    end

    subgraph "网关 (数据面)"
        B[接收请求]
        C[路由/鉴权]
        D{限流检查}
        E{熔断检查}
        F[转发请求]
        G[接收响应]
        H{是否异常}
        I[错误采样]
        J[脱敏处理]
        K[发送到Kafka]
        L[返回响应]
    end

    subgraph "消息队列"
        M[Kafka存储]
    end

    subgraph "控制面"
        N[消费错误事件]
        O[文本预处理]
        P[向量化]
        Q[相似度计算]
        R{找到相似簇?}
        S[加入现有簇]
        T[创建新簇]
        U[更新簇统计]
        V[策略评估]
        W{触发条件?}
        X[生成策略]
        Y[策略下发]
    end

    subgraph "配置中心"
        Z[ETCD存储策略]
        AA[通知网关更新]
    end

    A --> B
    B --> C
    C --> D
    D -->|通过| E
    D -->|限流| L
    E -->|通过| F
    E -->|熔断| L
    F --> G
    G --> H
    H -->|正常| L
    H -->|异常| I
    I --> J
    J --> K
    K --> M
    M --> N
    N --> O
    O --> P
    P --> Q
    Q --> R
    R -->|是| S
    R -->|否| T
    S --> U
    T --> U
    U --> V
    V --> W
    W -->|是| X
    W -->|否| V
    X --> Y
    Y --> Z
    Z --> AA
    AA --> D
    AA --> E
```

### E-R图设计

#### 数据库设计

```mermaid
erDiagram
    CLUSTERS {
        string cluster_id PK
        text centroid_vector
        int error_count
        float severity
        timestamp create_time
        timestamp update_time
        text description
    }

    ERROR_EVENTS {
        string event_id PK
        string trace_id
        string span_id
        string request_path
        string method
        string service_name
        int status_code
        text error_message
        text stack_trace
        timestamp timestamp
        string cluster_id FK
    }

    VECTORS {
        string vector_id PK
        text vector_data
        string event_id FK
        timestamp created_at
        timestamp updated_at
    }

    POLICIES {
        string policy_id PK
        string cluster_id FK
        string policy_type
        float severity
        text policy_config
        timestamp create_time
        timestamp expire_time
        boolean is_active
    }

    CLUSTER_MEMBERS {
        string cluster_id FK
        string event_id FK
        float similarity_score
        timestamp joined_at
    }

    METRICS {
        string metric_id PK
        string cluster_id FK
        string metric_name
        float metric_value
        timestamp timestamp
        text labels
    }

    CLUSTERS ||--o{ ERROR_EVENTS : contains
    ERROR_EVENTS ||--|| VECTORS : has
    CLUSTERS ||--o{ POLICIES : triggers
    CLUSTERS ||--o{ CLUSTER_MEMBERS : includes
    ERROR_EVENTS ||--o{ CLUSTER_MEMBERS : belongs_to
    CLUSTERS ||--o{ METRICS : generates
```

### 状态机设计

#### 熔断器状态转换图

```mermaid
stateDiagram-v2
    [*] --> CLOSED

    CLOSED --> OPEN : 失败次数 >= 阈值
    OPEN --> HALF_OPEN : 超时恢复
    HALF_OPEN --> CLOSED : 成功次数 >= 阈值
    HALF_OPEN --> OPEN : 失败次数 >= 阈值

    state CLOSED {
        [*] --> Monitoring
        Monitoring --> Recording_Success : 成功请求
        Monitoring --> Recording_Failure : 失败请求
        Recording_Success --> Monitoring
        Recording_Failure --> Monitoring
        Recording_Failure --> [*] : 失败计数达到阈值
    }

    state OPEN {
        [*] --> Rejecting_Requests
        Rejecting_Requests --> Waiting_Timeout : 开始等待
        Waiting_Timeout --> [*] : 超时
    }

    state HALF_OPEN {
        [*] --> Limited_Testing
        Limited_Testing --> Success_Count : 成功请求
        Limited_Testing --> Failure_Count : 失败请求
        Success_Count --> [*] : 成功次数足够
        Failure_Count --> [*] : 失败次数达到阈值
    }
```

#### 策略生命周期状态图

```mermaid
stateDiagram-v2
    [*] --> EVALUATING

    EVALUATING --> TRIGGERED : 触发条件满足
    EVALUATING --> MONITORING : 未触发

    TRIGGERED --> POLICY_GENERATED : 生成策略
    POLICY_GENERATED --> POLICY_APPLIED : 策略下发
    POLICY_APPLIED --> ACTIVE : 策略生效

    ACTIVE --> RECOVERING : 异常消退
    ACTIVE --> EXPIRED : 策略过期
    ACTIVE --> ESCALATING : 异常加重

    RECOVERING --> MONITORING : 完全恢复
    ESCALATING --> POLICY_UPDATED : 更新策略
    POLICY_UPDATED --> ACTIVE
    EXPIRED --> MONITORING

    MONITORING --> EVALUATING : 继续监控

    state ACTIVE {
        [*] --> Rate_Limiting
        [*] --> Circuit_Breaking
        [*] --> Degrading

        Rate_Limiting --> Adjusting_Rate : 动态调整
        Circuit_Breaking --> Half_Open_Testing : 半开测试
        Degrading --> Fallback_Response : 降级响应

        Adjusting_Rate --> Rate_Limiting
        Half_Open_Testing --> Circuit_Breaking
        Fallback_Response --> Degrading
    }
```

### 部署架构图

#### 生产环境部署拓扑

```mermaid
graph TB
    subgraph "负载均衡层"
        LB[负载均衡器 Nginx/Envoy]
    end

    subgraph "Kubernetes集群"
        subgraph "网关服务 (数据面)"
            GW1[Gateway Pod 1]
            GW2[Gateway Pod 2]
            GW3[Gateway Pod 3]
        end

        subgraph "控制面服务"
            CP1[Control Plane Pod 1]
            CP2[Control Plane Pod 2]
        end

        subgraph "中间件服务"
            ETCD_CLUSTER[ETCD集群]
            KAFKA_CLUSTER[Kafka集群]
        end
    end

    subgraph "存储层"
        PG_MASTER[(PostgreSQL主)]
        PG_SLAVE[(PostgreSQL从)]
        REDIS_CLUSTER[(Redis集群)]
    end

    subgraph "监控系统"
        PROM[Prometheus]
        GRAF[Grafana]
        ALERT[AlertManager]
    end

    subgraph "下游服务"
        SVC1[Service A]
        SVC2[Service B]
        SVC3[Service C]
    end

    LB --> GW1
    LB --> GW2
    LB --> GW3

    GW1 --> SVC1
    GW1 --> SVC2
    GW1 --> SVC3
    GW2 --> SVC1
    GW2 --> SVC2
    GW2 --> SVC3
    GW3 --> SVC1
    GW3 --> SVC2
    GW3 --> SVC3

    GW1 --> KAFKA_CLUSTER
    GW2 --> KAFKA_CLUSTER
    GW3 --> KAFKA_CLUSTER

    GW1 --> ETCD_CLUSTER
    GW2 --> ETCD_CLUSTER
    GW3 --> ETCD_CLUSTER

    KAFKA_CLUSTER --> CP1
    KAFKA_CLUSTER --> CP2

    CP1 --> ETCD_CLUSTER
    CP2 --> ETCD_CLUSTER

    CP1 --> PG_MASTER
    CP2 --> PG_MASTER

    CP1 --> REDIS_CLUSTER
    CP2 --> REDIS_CLUSTER

    PG_MASTER --> PG_SLAVE

    GW1 --> PROM
    GW2 --> PROM
    GW3 --> PROM
    CP1 --> PROM
    CP2 --> PROM

    PROM --> GRAF
    PROM --> ALERT
```

---

## 🔧 核心模块技术实现

### 1. 数据面 (Gateway) 技术实现

#### 1.1 网关核心框架

```go
// gateway/main.go
type Gateway struct {
    router         *gin.Engine
    rateLimiter    *RateLimiter
    circuitBreaker *CircuitBreaker
    errorSampler   *ErrorSampler
    vectorAgent    *VectorAgent
    configWatcher  *ConfigWatcher
}

// 中间件链路
func (g *Gateway) setupMiddleware() {
    g.router.Use(
        middleware.Recovery(),
        middleware.Logger(),
        middleware.Tracing(),           // OpenTelemetry追踪
        middleware.Authentication(),    // JWT/OIDC认证
        middleware.RateLimit(),        // 基于簇的限流
        middleware.CircuitBreaker(),   // 基于簇的熔断
        middleware.ErrorSampling(),    // 错误采样
        middleware.Metrics(),          // 指标收集
    )
}
```

#### 1.2 错误采样器实现

```go
// gateway/sampler/error_sampler.go
type ErrorSampler struct {
    samplingRate float64
    kafkaProducer *kafka.Producer
    desensitizer *Desensitizer
}

type ErrorEvent struct {
    TraceID      string    `json:"trace_id"`
    SpanID       string    `json:"span_id"`
    RequestPath  string    `json:"request_path"`
    Method       string    `json:"method"`
    ServiceName  string    `json:"service_name"`
    StatusCode   int       `json:"status_code"`
    ErrorMessage string    `json:"error_message"`
    StackTrace   []string  `json:"stack_trace"`
    Timestamp    time.Time `json:"timestamp"`
}

func (es *ErrorSampler) SampleError(ctx *gin.Context, err error) {
    // 采样判断
    if rand.Float64() > es.samplingRate {
        return
    }

    // 错误信息提取
    event := &ErrorEvent{
        TraceID:      extractTraceID(ctx),
        SpanID:       extractSpanID(ctx),
        RequestPath:  ctx.Request.URL.Path,
        Method:       ctx.Request.Method,
        ServiceName:  extractServiceName(ctx),
        StatusCode:   ctx.Writer.Status(),
        ErrorMessage: err.Error(),
        StackTrace:   extractStackTrace(err, 2), // 提取前2帧
        Timestamp:    time.Now(),
    }

    // 敏感信息脱敏
    event.ErrorMessage = es.desensitizer.Desensitize(event.ErrorMessage)

    // 异步发送到Kafka
    go es.sendToKafka(event)
}
```

#### 1.3 基于簇的限流器实现

```go
// gateway/limiter/cluster_rate_limiter.go
type ClusterRateLimiter struct {
    clusters map[string]*ClusterLimiter
    mutex    sync.RWMutex
    config   *ConfigWatcher
}

type ClusterLimiter struct {
    ClusterID    string
    TokenBucket  *TokenBucket
    Severity     float64  // 簇严重度 0.0-1.0
    BaseRate     float64  // 基础令牌速率
    CurrentRate  float64  // 当前令牌速率
}

func (crl *ClusterRateLimiter) Allow(ctx *gin.Context) bool {
    clusterID := crl.identifyCluster(ctx)
    if clusterID == "" {
        return true // 无法识别簇，放行
    }

    crl.mutex.RLock()
    limiter, exists := crl.clusters[clusterID]
    crl.mutex.RUnlock()

    if !exists {
        return true // 簇不存在限流策略，放行
    }

    // 基于簇严重度调整令牌速率
    adjustedRate := limiter.BaseRate * (1.0 - limiter.Severity)
    limiter.TokenBucket.SetRate(adjustedRate)

    return limiter.TokenBucket.Allow()
}

// 通过向量相似度识别请求所属簇
func (crl *ClusterRateLimiter) identifyCluster(ctx *gin.Context) string {
    // 从上下文提取错误特征（如果存在）
    errorSignature := extractErrorSignature(ctx)
    if errorSignature == "" {
        return ""
    }

    // 调用向量化Agent计算簇归属
    return crl.vectorAgent.IdentifyCluster(errorSignature)
}
```

#### 1.4 基于簇的熔断器实现

```go
// gateway/breaker/cluster_circuit_breaker.go
type ClusterCircuitBreaker struct {
    clusters map[string]*ClusterBreaker
    mutex    sync.RWMutex
}

type ClusterBreaker struct {
    ClusterID     string
    State         BreakerState // CLOSED, OPEN, HALF_OPEN
    FailureCount  int64
    SuccessCount  int64
    LastFailTime  time.Time
    NextRetry     time.Time
    Config        *BreakerConfig
}

type BreakerConfig struct {
    FailureThreshold  int64         // 失败次数阈值
    RecoveryTimeout   time.Duration // 恢复超时时间
    RecoveryIncrement float64       // 恢复增量 (20%)
}

func (ccb *ClusterCircuitBreaker) Allow(ctx *gin.Context, clusterID string) bool {
    ccb.mutex.Lock()
    breaker, exists := ccb.clusters[clusterID]
    if !exists {
        breaker = &ClusterBreaker{
            ClusterID: clusterID,
            State:     CLOSED,
            Config:    ccb.defaultConfig,
        }
        ccb.clusters[clusterID] = breaker
    }
    ccb.mutex.Unlock()

    switch breaker.State {
    case CLOSED:
        return true
    case OPEN:
        if time.Now().After(breaker.NextRetry) {
            breaker.State = HALF_OPEN
            return true
        }
        return false
    case HALF_OPEN:
        return true
    }

    return false
}
```

### 2. 控制面技术实现

#### 2.1 向量化服务

```go
// control-plane/embedding/embedding_service.go
type EmbeddingService struct {
    model     *BGEModel
    cache     *lru.Cache
    batchSize int
}

type BGEModel struct {
    modelPath string
    session   *onnxruntime.Session
}

func (es *EmbeddingService) EmbedText(text string) ([]float32, error) {
    // 检查缓存
    if cached, exists := es.cache.Get(text); exists {
        return cached.([]float32), nil
    }

    // 文本预处理
    processedText := es.preprocessText(text)

    // 调用BGE模型生成向量
    vector, err := es.model.Encode(processedText)
    if err != nil {
        return nil, err
    }

    // 缓存结果
    es.cache.Add(text, vector)

    return vector, nil
}

func (es *EmbeddingService) preprocessText(text string) string {
    // 模板化处理：将变量替换为占位符
    text = regexp.MustCompile(`\b\d{11}\b`).ReplaceAllString(text, "[PHONE]")
    text = regexp.MustCompile(`\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b`).ReplaceAllString(text, "[EMAIL]")
    text = regexp.MustCompile(`\b[A-Za-z0-9]{20,}\b`).ReplaceAllString(text, "[TOKEN]")

    return text
}

// 批量向量化处理
func (es *EmbeddingService) EmbedBatch(texts []string) ([][]float32, error) {
    vectors := make([][]float32, len(texts))

    // 分批处理
    for i := 0; i < len(texts); i += es.batchSize {
        end := i + es.batchSize
        if end > len(texts) {
            end = len(texts)
        }

        batch := texts[i:end]
        batchVectors, err := es.model.EncodeBatch(batch)
        if err != nil {
            return nil, err
        }

        copy(vectors[i:end], batchVectors)
    }

    return vectors, nil
}
```

#### 2.2 聚类引擎实现

```go
// control-plane/clustering/clustering_engine.go
type ClusteringEngine struct {
    vectorDB      *VectorDB
    clusterIndex  *faiss.Index
    clusters      map[string]*Cluster
    similarityThreshold float64 // 0.82
    mutex         sync.RWMutex
}

type Cluster struct {
    ID          string
    Centroid    []float32
    Members     []string
    ErrorCount  int64
    CreateTime  time.Time
    UpdateTime  time.Time
    Severity    float64
}

func (ce *ClusteringEngine) ProcessErrorEvent(event *ErrorEvent) error {
    // 生成错误特征文本
    featureText := ce.generateFeatureText(event)

    // 向量化
    vector, err := ce.embeddingService.EmbedText(featureText)
    if err != nil {
        return err
    }

    // 查找最相似的簇
    clusterID, similarity := ce.findMostSimilarCluster(vector)

    if similarity >= ce.similarityThreshold {
        // 归入现有簇
        ce.addToCluster(clusterID, event, vector)
    } else {
        // 创建新簇
        newClusterID := ce.createNewCluster(event, vector)
        log.Printf("创建新异常簇: %s", newClusterID)
    }

    return nil
}

func (ce *ClusteringEngine) findMostSimilarCluster(vector []float32) (string, float64) {
    ce.mutex.RLock()
    defer ce.mutex.RUnlock()

    var bestClusterID string
    var bestSimilarity float64

    for clusterID, cluster := range ce.clusters {
        similarity := ce.cosineSimilarity(vector, cluster.Centroid)
        if similarity > bestSimilarity {
            bestSimilarity = similarity
            bestClusterID = clusterID
        }
    }

    return bestClusterID, bestSimilarity
}

func (ce *ClusteringEngine) cosineSimilarity(a, b []float32) float64 {
    var dotProduct, normA, normB float64

    for i := range a {
        dotProduct += float64(a[i]) * float64(b[i])
        normA += float64(a[i]) * float64(a[i])
        normB += float64(b[i]) * float64(b[i])
    }

    return dotProduct / (math.Sqrt(normA) * math.Sqrt(normB))
}

// 定期重聚类优化
func (ce *ClusteringEngine) ReCluster() {
    ce.mutex.Lock()
    defer ce.mutex.Unlock()

    // 收集所有向量
    var vectors [][]float32
    var memberIDs []string

    for _, cluster := range ce.clusters {
        for _, memberID := range cluster.Members {
            vector, err := ce.vectorDB.GetVector(memberID)
            if err != nil {
                continue
            }
            vectors = append(vectors, vector)
            memberIDs = append(memberIDs, memberID)
        }
    }

    // 使用DBSCAN重新聚类
    newClusters := ce.dbscanClustering(vectors, memberIDs)

    // 更新簇信息
    ce.clusters = newClusters

    log.Printf("重聚类完成，当前簇数量: %d", len(ce.clusters))
}
```

#### 2.3 策略引擎实现

```go
// control-plane/policy/policy_engine.go
type PolicyEngine struct {
    clusters      map[string]*Cluster
    policies      map[string]*Policy
    configStore   *ConfigStore
    metrics       *MetricsCollector
}

type Policy struct {
    ClusterID     string
    PolicyType    PolicyType // RATE_LIMIT, CIRCUIT_BREAK, DEGRADE
    Severity      float64
    RateLimit     *RateLimitPolicy
    CircuitBreak  *CircuitBreakPolicy
    CreateTime    time.Time
    ExpireTime    time.Time
}

type RateLimitPolicy struct {
    LimitRate    float64  // 限制比例 0.0-1.0
    Duration     time.Duration
}

type CircuitBreakPolicy struct {
    BreakDuration time.Duration
    RecoveryStep  float64 // 恢复步长
}

func (pe *PolicyEngine) EvaluatePolicies() {
    for clusterID, cluster := range pe.clusters {
        // 计算时间窗口内的错误指标
        windowSize := 10 * time.Second
        errorRate := pe.calculateErrorRate(clusterID, windowSize)
        growthRate := pe.calculateGrowthRate(clusterID, windowSize)

        // 判断是否需要触发策略
        shouldTrigger := pe.shouldTriggerPolicy(errorRate, growthRate)

        if shouldTrigger {
            policy := pe.generatePolicy(cluster, errorRate, growthRate)
            pe.applyPolicy(policy)

            log.Printf("为簇 %s 生成策略: 类型=%v, 严重度=%.2f",
                clusterID, policy.PolicyType, policy.Severity)
        }
    }
}

func (pe *PolicyEngine) shouldTriggerPolicy(errorRate, growthRate float64) bool {
    // PRD中定义的触发条件
    errorThreshold := 0.20  // 20%
    growthThreshold := 50.0 // 50次/10s

    return errorRate >= errorThreshold && growthRate >= growthThreshold
}

func (pe *PolicyEngine) generatePolicy(cluster *Cluster, errorRate, growthRate float64) *Policy {
    // 根据错误率和增长率计算严重度
    severity := pe.calculateSeverity(errorRate, growthRate)

    var policyType PolicyType
    if severity >= 0.8 {
        policyType = CIRCUIT_BREAK
    } else if severity >= 0.4 {
        policyType = RATE_LIMIT
    } else {
        policyType = DEGRADE
    }

    policy := &Policy{
        ClusterID:  cluster.ID,
        PolicyType: policyType,
        Severity:   severity,
        CreateTime: time.Now(),
        ExpireTime: time.Now().Add(5 * time.Minute),
    }

    switch policyType {
    case RATE_LIMIT:
        policy.RateLimit = &RateLimitPolicy{
            LimitRate: severity * 0.8, // 最大限制80%
            Duration:  time.Minute,
        }
    case CIRCUIT_BREAK:
        policy.CircuitBreak = &CircuitBreakPolicy{
            BreakDuration: 30 * time.Second,
            RecoveryStep:  0.2, // 每次恢复20%
        }
    }

    return policy
}

// 策略下发到数据面
func (pe *PolicyEngine) applyPolicy(policy *Policy) {
    // 将策略写入ETCD
    key := fmt.Sprintf("/policies/%s", policy.ClusterID)
    value, _ := json.Marshal(policy)

    pe.configStore.Put(key, string(value))

    // 记录指标
    pe.metrics.PolicyApplied.WithLabelValues(
        policy.ClusterID,
        string(policy.PolicyType),
    ).Inc()
}
```

### 3. 向量数据库设计

```go
// control-plane/vectordb/vector_db.go
type VectorDB struct {
    faissIndex *faiss.Index
    pgConn     *sql.DB
    cache      *lru.Cache
}

func NewVectorDB() *VectorDB {
    // 创建FAISS索引 (内存)
    dimension := 768 // BGE-small向量维度
    index := faiss.NewIndexFlatIP(dimension)

    // PostgreSQL连接 (持久化)
    pgConn, _ := sql.Open("postgres", "postgres://user:pass@localhost/vectordb")

    return &VectorDB{
        faissIndex: index,
        pgConn:     pgConn,
        cache:      lru.New(10000),
    }
}

func (vdb *VectorDB) AddVector(id string, vector []float32) error {
    // 添加到FAISS索引
    vdb.faissIndex.Add([][]float32{vector})

    // 持久化到PostgreSQL
    _, err := vdb.pgConn.Exec(`
        INSERT INTO vectors (id, vector, created_at)
        VALUES ($1, $2, NOW())
        ON CONFLICT (id) DO UPDATE SET
        vector = $2, updated_at = NOW()
    `, id, pq.Array(vector))

    return err
}

func (vdb *VectorDB) SearchSimilar(query []float32, topK int) ([]SearchResult, error) {
    // 使用FAISS进行相似度搜索
    distances, ids, err := vdb.faissIndex.Search([][]float32{query}, topK)
    if err != nil {
        return nil, err
    }

    results := make([]SearchResult, len(ids[0]))
    for i, id := range ids[0] {
        results[i] = SearchResult{
            ID:         fmt.Sprintf("%d", id),
            Similarity: float64(distances[0][i]),
        }
    }

    return results, nil
}
```

---

## 📊 数据流程设计

### 错误处理流程

```
1. 请求异常 → Gateway错误处理中间件
2. 错误采样 → 敏感信息脱敏
3. Kafka异步发送 → 控制面消费
4. 文本向量化 → 向量相似度计算
5. 簇归属判断 → 更新簇统计信息
6. 策略评估 → 生成限流/熔断策略
7. ETCD策略下发 → Gateway配置更新
8. 精确限流执行 → 保护下游服务
```

### 配置更新流程

```go
// gateway/config/config_watcher.go
type ConfigWatcher struct {
    etcdClient *etcd.Client
    policies   map[string]*Policy
    callbacks  []ConfigUpdateCallback
    mutex      sync.RWMutex
}

func (cw *ConfigWatcher) WatchPolicyUpdates() {
    watchChan := cw.etcdClient.Watch(context.Background(), "/policies/", etcd.WithPrefix())

    for watchResp := range watchChan {
        for _, event := range watchResp.Events {
            clusterID := strings.TrimPrefix(string(event.Kv.Key), "/policies/")

            switch event.Type {
            case etcd.EventTypePut:
                var policy Policy
                json.Unmarshal(event.Kv.Value, &policy)

                cw.mutex.Lock()
                cw.policies[clusterID] = &policy
                cw.mutex.Unlock()

                // 通知所有监听器
                for _, callback := range cw.callbacks {
                    callback.OnPolicyUpdate(clusterID, &policy)
                }

            case etcd.EventTypeDelete:
                cw.mutex.Lock()
                delete(cw.policies, clusterID)
                cw.mutex.Unlock()

                for _, callback := range cw.callbacks {
                    callback.OnPolicyDelete(clusterID)
                }
            }
        }
    }
}
```

---

## 🚀 性能优化方案

### 1. 向量计算优化

```go
// 向量批处理优化
type BatchProcessor struct {
    batchSize   int
    maxWaitTime time.Duration
    buffer      []*ErrorEvent
    mutex       sync.Mutex
}

func (bp *BatchProcessor) AddEvent(event *ErrorEvent) {
    bp.mutex.Lock()
    defer bp.mutex.Unlock()

    bp.buffer = append(bp.buffer, event)

    if len(bp.buffer) >= bp.batchSize {
        go bp.processBatch(bp.buffer[:bp.batchSize])
        bp.buffer = bp.buffer[bp.batchSize:]
    }
}

// SIMD优化的余弦相似度计算
func CosineSimilaritySIMD(a, b []float32) float64 {
    // 使用AVX指令集优化向量运算
    return simd.CosineSimilarity(a, b)
}
```

### 2. 缓存优化策略

```go
type MultiLevelCache struct {
    l1Cache *sync.Map        // 热点数据
    l2Cache *lru.Cache       // LRU缓存
    l3Cache *redis.Client    // Redis分布式缓存
}

func (mlc *MultiLevelCache) Get(key string) (interface{}, bool) {
    // L1缓存查找
    if value, ok := mlc.l1Cache.Load(key); ok {
        return value, true
    }

    // L2缓存查找
    if value, ok := mlc.l2Cache.Get(key); ok {
        mlc.l1Cache.Store(key, value) // 提升到L1
        return value, true
    }

    // L3缓存查找
    value, err := mlc.l3Cache.Get(key).Result()
    if err == nil {
        mlc.l2Cache.Add(key, value)    // 提升到L2
        mlc.l1Cache.Store(key, value)  // 提升到L1
        return value, true
    }

    return nil, false
}
```

### 3. 索引优化

```go
// 分层索引结构
type HierarchicalIndex struct {
    coarseIndex *faiss.Index  // 粗粒度索引
    fineIndex   *faiss.Index  // 细粒度索引
    threshold   float64
}

func (hi *HierarchicalIndex) Search(query []float32, topK int) ([]SearchResult, error) {
    // 第一阶段：粗粒度搜索
    coarseResults, err := hi.coarseIndex.Search([][]float32{query}, topK*2)
    if err != nil {
        return nil, err
    }

    // 第二阶段：细粒度精确搜索
    var candidates [][]float32
    for _, id := range coarseResults[0] {
        vector, _ := hi.getVector(int64(id))
        candidates = append(candidates, vector)
    }

    fineResults, err := hi.fineIndex.Search(candidates, topK)
    return hi.convertResults(fineResults), err
}
```

---

## 📈 监控与可观测性

### 监控指标设计

```go
// monitoring/metrics.go
var (
    // 网关指标
    RequestTotal = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "gateway_requests_total",
            Help: "Total number of requests",
        },
        []string{"method", "path", "status", "cluster_id"},
    )

    RequestDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "gateway_request_duration_seconds",
            Help:    "Request duration in seconds",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "path", "cluster_id"},
    )

    // 限流指标
    RateLimitHits = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "rate_limit_hits_total",
            Help: "Total number of rate limit hits",
        },
        []string{"cluster_id", "policy_type"},
    )

    // 熔断指标
    CircuitBreakerState = prometheus.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "circuit_breaker_state",
            Help: "Circuit breaker state (0=closed, 1=open, 2=half-open)",
        },
        []string{"cluster_id"},
    )

    // 聚类指标
    ClusterSize = prometheus.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "cluster_size",
            Help: "Number of errors in cluster",
        },
        []string{"cluster_id"},
    )

    ClusterSeverity = prometheus.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "cluster_severity",
            Help: "Cluster severity score",
        },
        []string{"cluster_id"},
    )
)
```

### 链路追踪实现

```go
// tracing/tracer.go
func TracingMiddleware() gin.HandlerFunc {
    return gin.HandlerFunc(func(c *gin.Context) {
        tracer := otel.Tracer("llm-aware-gateway")

        ctx, span := tracer.Start(c.Request.Context(), "gateway.request")
        defer span.End()

        // 添加追踪属性
        span.SetAttributes(
            attribute.String("http.method", c.Request.Method),
            attribute.String("http.path", c.Request.URL.Path),
            attribute.String("user.agent", c.Request.UserAgent()),
        )

        // 注入追踪上下文
        c.Request = c.Request.WithContext(ctx)

        c.Next()

        // 记录响应状态
        span.SetAttributes(
            attribute.Int("http.status_code", c.Writer.Status()),
        )

        if c.Writer.Status() >= 400 {
            span.SetStatus(codes.Error, "Request failed")

            // 如果是错误请求，添加簇信息
            if clusterID := c.GetString("cluster_id"); clusterID != "" {
                span.SetAttributes(
                    attribute.String("cluster.id", clusterID),
                )
            }
        }
    })
}
```

---

## 🛠️ 部署方案

### Docker容器化

```dockerfile
# Dockerfile.gateway
FROM golang:1.21-alpine AS builder

WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download

COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -o gateway ./cmd/gateway

FROM alpine:latest
RUN apk --no-cache add ca-certificates tzdata
WORKDIR /root/

COPY --from=builder /app/gateway .
COPY --from=builder /app/configs ./configs

EXPOSE 8080
CMD ["./gateway", "--config", "configs/gateway.yaml"]
```

### Kubernetes部署

```yaml
# deploy/gateway-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-aware-gateway
  namespace: gateway
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-aware-gateway
  template:
    metadata:
      labels:
        app: llm-aware-gateway
    spec:
      containers:
      - name: gateway
        image: llm-aware-gateway:latest
        ports:
        - containerPort: 8080
        env:
        - name: ETCD_ENDPOINTS
          value: "etcd:2379"
        - name: KAFKA_BROKERS
          value: "kafka:9092"
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: gateway-service
  namespace: gateway
spec:
  selector:
    app: llm-aware-gateway
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
```

### Helm Chart

```yaml
# helm/values.yaml
gateway:
  replicas: 3
  image:
    repository: llm-aware-gateway
    tag: latest

  config:
    sampling_rate: 0.05
    similarity_threshold: 0.82
    error_rate_threshold: 0.20
    growth_rate_threshold: 50

  resources:
    requests:
      memory: "256Mi"
      cpu: "200m"
    limits:
      memory: "512Mi"
      cpu: "500m"

controlplane:
  replicas: 2
  image:
    repository: llm-aware-controlplane
    tag: latest

  embedding:
    model_path: "/models/bge-small-zh"
    batch_size: 50
    cache_size: 10000

  clustering:
    reclustering_interval: "5m"
    min_cluster_size: 10

etcd:
  enabled: true
  replicas: 3

kafka:
  enabled: true
  replicas: 3
  topics:
    - name: error-events
      partitions: 6
      replicas: 2

postgresql:
  enabled: true
  replicas: 1
  extensions:
    - vector
```

---

## 🧪 测试方案

### 单元测试

```go
// gateway/limiter/limiter_test.go
func TestClusterRateLimiter(t *testing.T) {
    limiter := NewClusterRateLimiter()

    // 测试基础限流
    cluster := &ClusterLimiter{
        ClusterID:   "test-cluster",
        TokenBucket: NewTokenBucket(100), // 100 TPS
        Severity:    0.5,                 // 50% 严重度
        BaseRate:    100,
    }
    limiter.clusters["test-cluster"] = cluster

    ctx := &gin.Context{}
    ctx.Set("cluster_id", "test-cluster")

    // 前50个请求应该被允许 (100 * (1-0.5) = 50 TPS)
    allowed := 0
    for i := 0; i < 100; i++ {
        if limiter.Allow(ctx) {
            allowed++
        }
    }

    assert.Equal(t, 50, allowed)
}
```

### 集成测试

```go
// test/integration_test.go
func TestEndToEndFlow(t *testing.T) {
    // 启动测试环境
    testEnv := setupTestEnvironment()
    defer testEnv.Cleanup()

    // 1. 发送正常请求
    resp := testEnv.SendRequest("GET", "/api/users", nil)
    assert.Equal(t, 200, resp.StatusCode)

    // 2. 模拟错误请求
    for i := 0; i < 100; i++ {
        testEnv.SendRequest("GET", "/api/error-endpoint", nil)
    }

    // 3. 等待聚类和策略生成
    time.Sleep(30 * time.Second)

    // 4. 验证限流策略生效
    rateLimited := 0
    for i := 0; i < 50; i++ {
        resp := testEnv.SendRequest("GET", "/api/error-endpoint", nil)
        if resp.StatusCode == 429 {
            rateLimited++
        }
    }

    assert.Greater(t, rateLimited, 10, "应该有部分请求被限流")
}
```

### 压力测试

```bash
#!/bin/bash
# test/stress_test.sh

# 使用wrk进行压力测试
wrk -t12 -c400 -d30s --script=lua/stress_test.lua http://gateway:8080/api/test

# 监控关键指标
while true; do
    echo "=== $(date) ==="
    curl -s http://gateway:8080/metrics | grep -E "(request_duration|rate_limit_hits|cluster_size)"
    sleep 10
done
```

---

## 🔄 发布与运维

### 灰度发布策略

```yaml
# 使用Istio进行灰度发布
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: gateway-canary
spec:
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: gateway-service
        subset: canary
      weight: 100
  - route:
    - destination:
        host: gateway-service
        subset: stable
      weight: 90
    - destination:
        host: gateway-service
        subset: canary
      weight: 10
```

### 监控告警

```yaml
# 关键指标告警规则
groups:
- name: llm-aware-gateway
  rules:
  - alert: HighErrorRate
    expr: rate(gateway_requests_total{status=~"5.."}[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "网关错误率过高"
      description: "错误率: {{ $value }}"

  - alert: ClusterSizeAnomaly
    expr: cluster_size > 1000
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "异常簇规模过大"
      description: "簇 {{ $labels.cluster_id }} 规模: {{ $value }}"

  - alert: PolicyEngineDown
    expr: up{job="control-plane"} == 0
    for: 30s
    labels:
      severity: critical
    annotations:
      summary: "控制面服务下线"
```

### 自动扩缩容

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gateway-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-aware-gateway
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
```

---

## 🎯 总结

本技术方案详细描述了LLM-Aware Gateway的完整实现方案，包括：

1. **分层架构设计**：数据面负责高性能流量处理，控制面负责智能决策
2. **核心算法实现**：基于向量相似度的聚类算法，精确识别同类异常
3. **性能优化**：多级缓存、批处理、SIMD优化等手段保证低延迟
4. **可观测性**：完整的监控、链路追踪、告警体系
5. **云原生部署**：基于Kubernetes的容器化部署，支持弹性伸缩

该方案能够实现PRD中定义的核心目标：
- 减灾效果：同源故障5xx峰值下降≥40%
- 性能开销：P95延迟增加≤2ms
- 误杀率：被限流但实际成功≤2%
- 恢复速度：故障消除60s内恢复

通过语义感知的智能熔断限流，显著提升系统稳定性和用户体验。
